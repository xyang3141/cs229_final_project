{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c3535bdb-aa93-4191-b841-84a91ec7700d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0ca06ad3-a29c-49b2-9286-894b1933cf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.legacy import data\n",
    "from torchtext.legacy import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b264a163-7d8c-4846-9c80-1c9e0bc84d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "53aec1a5-8f8d-46e2-8956-27bda8973766",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_LAUNCH_BLOCKING=1\n",
    "import pandas as pd\n",
    "# import spacy\n",
    "import numpy as np\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "test_df = pd.read_csv(\"bert_features_sentiment.csv\", header=None)\n",
    "\n",
    "X = test_df[0]\n",
    "Y_true = test_df[1]\n",
    "pred_res = []\n",
    "sentences = [sentence for i, sentence in X.items()]\n",
    "sentence_embeddings = model.encode(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44564de8-6fde-4bc3-ad54-f24e6b6566b9",
   "metadata": {
    "id": "wdlQme113c7n"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "label_new =[]\n",
    "for i in range(len(Y_true)):\n",
    "    # if tweet[i] is not np.nan:\n",
    "    #     # print(tweet[i])\n",
    "    if Y_true[i] == 1:\n",
    "        label_new.append(0)\n",
    "    elif Y_true[i] == 2:\n",
    "        label_new.append(1)\n",
    "    elif Y_true[i] == 3:\n",
    "        label_new.append(2)\n",
    "    elif Y_true[i] == 4:\n",
    "        label_new.append(3)\n",
    "    elif Y_true[i] == 5:\n",
    "        label_new.append(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d569cd-1a03-4321-94dc-42a7ecf5dec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38e1397-f07f-4d6e-88fc-9446db71ebe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "df6448c2-b10b-40a9-a0ec-6fe7cdb37c04",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YDyJDT34UGnI",
    "outputId": "a46ae2a7-9471-4e42-f8c4-97fd09d1dfdd"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "# from keras.utils import objectives\n",
    "import numpy as np\n",
    "from math import floor\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, LSTM, RepeatVector, Layer\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "# from keras.optimizers import SGD, RMSprop, Adam\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# from keras import objectives\n",
    "# import tensorflow.contrib.keras as keras\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "\n",
    "\n",
    "\n",
    "class VAE:\n",
    "    def __init__(self, input_dim, latent_dim, hidden_dims, batch_size, optimizer='rmsprop', epsilon_std = .01):\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = optimizer\n",
    "        self.epsilon_std = epsilon_std\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        input_layer = Input(batch_shape=(self.batch_size, self.input_dim))\n",
    "        self.build_encoder(input_layer)\n",
    "        self.build_decoder()\n",
    "        self.autoencoder = Model(input_layer, self.x_decoded_mean)\n",
    "        vae_loss = self._get_vae_loss()\n",
    "        self.autoencoder.compile(optimizer=self.optimizer, loss=vae_loss)\n",
    "\n",
    "    def build_encoder(self, input_layer):\n",
    "        prev_layer = input_layer\n",
    "        for q in self.hidden_dims:\n",
    "            hidden = Dense(q, activation='relu')(prev_layer)\n",
    "            prev_layer = hidden\n",
    "        self._build_z_layers(hidden)\n",
    "        self.encoder = Model(input_layer, self.z_mean)\n",
    "\n",
    "    def _build_z_layers(self, hidden_layer):\n",
    "        self.z_mean = Dense(self.latent_dim)(hidden_layer)\n",
    "        self.z_log_sigma = Dense(self.latent_dim)(hidden_layer)\n",
    "\n",
    "    def build_decoder(self):\n",
    "        z = self._get_sampling_layer()\n",
    "        prev_layer = z\n",
    "        for q in self.hidden_dims:\n",
    "            hidden = Dense(q, activation='relu')(prev_layer)\n",
    "            prev_layer = hidden\n",
    "        self.x_decoded_mean = Dense(self.input_dim, activation='sigmoid')(prev_layer)\n",
    "\n",
    "        # Build the stand-alone generator\n",
    "        generator_input = Input((self.latent_dim,))\n",
    "        prev_layer = generator_input\n",
    "        for q in self.hidden_dims:\n",
    "            hidden = Dense(q, activation='relu')(prev_layer)\n",
    "            prev_layer = hidden\n",
    "        gen_x_decoded_mean = Dense(self.input_dim, activation='sigmoid')(prev_layer)\n",
    "        self.generator = Model(generator_input, gen_x_decoded_mean)\n",
    "\n",
    "    def _get_sampling_layer(self):\n",
    "        def sampling(args):\n",
    "            z_mean, z_log_sigma = args\n",
    "            epsilon = K.random_normal(shape=(self.batch_size, self.latent_dim),\n",
    "                                      mean=0., stddev=self.epsilon_std)\n",
    "            return z_mean + z_log_sigma * epsilon\n",
    "        return Lambda(sampling, output_shape=(self.latent_dim,))([self.z_mean, self.z_log_sigma])\n",
    "\n",
    "    def _get_vae_loss(self):\n",
    "        z_log_sigma = self.z_log_sigma\n",
    "        z_mean = self.z_mean\n",
    "        def vae_loss(x, x_decoded_mean):\n",
    "            reconstruction_loss = tf.keras.metrics.mean_squared_error(x, x_decoded_mean)\n",
    "            kl_loss = - 0.5 * K.mean(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma))\n",
    "            return reconstruction_loss + kl_loss\n",
    "\n",
    "        return vae_loss\n",
    "\n",
    "\n",
    "class VAE_LSTM(VAE):\n",
    "    def __init__(self, input_dim, latent_dim, hidden_dims, timesteps, batch_size, optimizer='rmsprop', epsilon_std = .01):\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.batch_size = batch_size\n",
    "        self.timesteps = timesteps\n",
    "        self.optimizer = optimizer\n",
    "        self.epsilon_std = epsilon_std\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        input_layer = Input(shape=(self.timesteps, self.input_dim,))\n",
    "        self.build_encoder(input_layer)\n",
    "        self.build_decoder()\n",
    "        self.autoencoder = Model(input_layer, self.x_decoded_mean)\n",
    "        vae_loss = self._get_vae_loss()\n",
    "        self.autoencoder.compile(optimizer=self.optimizer, loss=vae_loss)\n",
    "\n",
    "    def build_encoder(self, input_layer):\n",
    "        prev_layer = input_layer\n",
    "        for q in self.hidden_dims:\n",
    "            hidden = LSTM(q)(prev_layer)\n",
    "            prev_layer = hidden\n",
    "        self._build_z_layers(hidden)\n",
    "        self.encoder = Model(input_layer, self.z_mean)\n",
    "\n",
    "    def build_decoder(self):\n",
    "        z = self._get_sampling_layer()\n",
    "        prev_layer = RepeatVector(self.timesteps)(z)\n",
    "        for q in self.hidden_dims:\n",
    "            hidden = LSTM(q, return_sequences=True)(prev_layer)\n",
    "            prev_layer = hidden\n",
    "        self.x_decoded_mean = LSTM(self.input_dim, return_sequences=True)(prev_layer)\n",
    "\n",
    "        # Build the stand-alone generator\n",
    "        generator_input = Input((self.latent_dim,))\n",
    "        prev_layer = RepeatVector(self.timesteps)(generator_input)\n",
    "        for q in self.hidden_dims:\n",
    "            hidden = LSTM(q, return_sequences=True)(prev_layer)\n",
    "            prev_layer = hidden\n",
    "        gen_x_decoded_mean = LSTM(self.input_dim, return_sequences=True)(prev_layer)\n",
    "        self.generator = Model(generator_input, gen_x_decoded_mean)\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d051cd0d-5055-46c6-aa55-8f42f77afbd1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "N = len(sentence_embeddings)\n",
    "disable_eager_execution()\n",
    "train = np.array(sentence_embeddings).astype(float)\n",
    "train = train.reshape([N,1,768])\n",
    "\n",
    "batch_size = 50\n",
    "epochs = 300\n",
    "input_dim = train.shape[-1]\n",
    "timesteps = train.shape[1]\n",
    "\n",
    "model = VAE_LSTM(input_dim=input_dim, latent_dim=100, hidden_dims=[32], timesteps=timesteps, batch_size=batch_size)\n",
    "vae, encoder, generator = model.autoencoder, model.encoder, model.generator\n",
    "#tf.compat.v1.disable_eager_execution()\n",
    "vae.fit(train[:floor(N/batch_size)*batch_size],train[:floor(N/batch_size)*batch_size], shuffle=True, epochs=epochs, batch_size=batch_size, validation_data=(train[N-1-batch_size:N-1],train[N-1-batch_size:N-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c50912ea-ac1a-4de3-9455-e97e550d66fa",
   "metadata": {
    "id": "uYOvjfYcUOYQ"
   },
   "outputs": [],
   "source": [
    "vector_vae = encoder.predict(np.array(train), batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "893cd941-c22d-4a5a-8085-6221f2617c5a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RBsYehjWg7nZ",
    "outputId": "2ddcfb71-bcc4-46b7-cfd1-a8ddda656a4f"
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import ADASYN \n",
    "\n",
    "ada = ADASYN(random_state=42)\n",
    "vector_vae_balanced, label_new_balanced = ada.fit_resample(vector_vae, label_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "27d9c2f3-4ae9-4f31-9bcd-0f81e633f9ad",
   "metadata": {
    "id": "1fz_bO1QUWco"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(vector_vae_balanced).to_csv('vector_vae_balanced.csv', index=False)\n",
    "pd.DataFrame(label_new_balanced).to_csv('label_new_balanced.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ee30d27-bb7a-45fa-b937-25a994d1eb28",
   "metadata": {
    "id": "Gxn1wjSzUZWg"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, n_hidden_1, n_hidden_2, out_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.droprate = 0.95\n",
    "        self.layer1 = nn.Sequential(nn.Linear(in_dim, n_hidden_1), nn.Dropout(p=self.droprate), nn.BatchNorm1d(n_hidden_1), nn.ReLU(True))\n",
    "        self.layer2 = nn.Sequential(nn.Linear(n_hidden_1, n_hidden_2), nn.Dropout(p=self.droprate), nn.BatchNorm1d(n_hidden_2), nn.ReLU(True))\n",
    "        self.layer3 = nn.Sequential(nn.Linear(n_hidden_2, out_dim), nn.Dropout(p=self.droprate))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data = torch.from_numpy(pd.read_csv('vector_vae_balanced.csv').values).float()\n",
    "        self.labels = torch.from_numpy(pd.read_csv('label_new_balanced.csv').values).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx,:]\n",
    "        labels = self.labels[idx]\n",
    "        return sample, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64744ad9-a7db-4145-b98a-8a1a76c7e0f4",
   "metadata": {
    "id": "9tDOq8a4UcAv"
   },
   "outputs": [],
   "source": [
    "def process(X_train, X_test, y_train, y_test):\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    X_train = np.array(X_train)\n",
    "    X_test = np.array(X_test)\n",
    "    Y_train = np.array(y_train)\n",
    "    Y_test = np.array(y_test)\n",
    "    X_train = torch.from_numpy(X_train).float()\n",
    "    X_test = torch.from_numpy(X_test).float()\n",
    "    Y_train = torch.from_numpy(Y_train).squeeze().to(torch.int64)\n",
    "    Y_test = torch.from_numpy(Y_test).squeeze().to(torch.int64)\n",
    "    train_dataset = []\n",
    "    test_dataset = []\n",
    "    for i in range(len(X_train)):\n",
    "        train_dataset.append((X_train[i],Y_train[i]))\n",
    "    for i in range(len(X_test)):\n",
    "        test_dataset.append((X_test[i],Y_test[i]))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "#do not standardize the one hot features\n",
    "def process_sentiment(X_train, X_test, y_train, y_test):\n",
    "    sentiment_train = X_train[:,len(X_train[0])-5:len(X_train[0])]\n",
    "    sentiment_test = X_test[:,len(X_test[0])-5:len(X_test[0])]\n",
    "    X_train = X_train[:,:len(X_train[0])-5]\n",
    "    X_test = X_test[:,:len(X_test[0])-5]\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    X_train = np.hstack((X_train, sentiment_train))\n",
    "    X_test = np.hstack((X_test, sentiment_test))\n",
    "    Y_train = np.array(y_train)\n",
    "    Y_test = np.array(y_test)\n",
    "    X_train = torch.from_numpy(X_train).float()\n",
    "    X_test = torch.from_numpy(X_test).float()\n",
    "    Y_train = torch.from_numpy(Y_train).squeeze().to(torch.int64)\n",
    "    Y_test = torch.from_numpy(Y_test).squeeze().to(torch.int64)\n",
    "    train_dataset = []\n",
    "    test_dataset = []\n",
    "    for i in range(len(X_train)):\n",
    "        train_dataset.append((X_train[i],Y_train[i]))\n",
    "    for i in range(len(X_test)):\n",
    "        test_dataset.append((X_test[i],Y_test[i]))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def classifier(input_dim, train_loader, test_loader, totEpoch, num_class, len_test):\n",
    "    model = Net(input_dim, 100, 30, 4)\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    epoch = 0\n",
    "    train_loss_list = []\n",
    "    loss_list = []\n",
    "    accuracy_list = []\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    F1_list = []\n",
    "    F1_list_train = []\n",
    "    TPR0_list=[]\n",
    "    FPR0_list=[]\n",
    "    y_pred_list=[]\n",
    "    lable_list=[]\n",
    "\n",
    "    for epoch2 in range(0, 0 + totEpoch):\n",
    "      # model.eval()\n",
    "        num_TP = [0]*num_class\n",
    "        num_FP = [0]*num_class\n",
    "        num_FN = [0]*num_class\n",
    "        eval_loss = 0\n",
    "        eval_acc = 0\n",
    "        eval_TP = [0]*num_class\n",
    "        eval_FP = [0]*num_class\n",
    "        eval_FN = [0]*num_class\n",
    "        precision = [0]*num_class\n",
    "        recall = [0]*num_class\n",
    "        F1 = [0]*num_class\n",
    "        for data in train_loader:\n",
    "            img, label = data\n",
    "            if torch.cuda.is_available():\n",
    "                img = img.cuda()\n",
    "                label = label.cuda()\n",
    "            else:\n",
    "                img = Variable(img)\n",
    "                label = Variable(label)\n",
    "            out = model(img)\n",
    "            loss = criterion(out, label)\n",
    "            # print_loss = loss.data.item()\n",
    "            _, pred = torch.max(out, 1)\n",
    "            num_correct = (pred == label).sum()\n",
    "\n",
    "            for i in range(num_class):\n",
    "                num_TP[i] = (((pred == i) & (label == i))).sum()\n",
    "                num_FP[i] = (((pred == i) & (label != i))).sum()\n",
    "                num_FN[i] = (((pred != i) & (label == i))).sum()\n",
    "                eval_TP[i] += num_TP[i].item()\n",
    "                eval_FP[i] += num_FP[i].item()\n",
    "                eval_FN[i] += num_FN[i].item()\n",
    "            eval_acc += num_correct.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch+=1\n",
    "\n",
    "        for i in range(num_class):\n",
    "            if (eval_TP[i] + eval_FP[i]) == 0:\n",
    "                precision[i] = 1\n",
    "            else:\n",
    "                precision[i] = eval_TP[i] / (eval_TP[i] + eval_FP[i])\n",
    "            if (eval_TP[i] + eval_FN[i]) == 0:\n",
    "                recall[i] = 1\n",
    "            else:\n",
    "                recall[i] = eval_TP[i] / (eval_TP[i] + eval_FN[i])\n",
    "            if (precision[i]+recall[i]) == 0:\n",
    "                F1[i] = 0\n",
    "            else:\n",
    "                F1[i] = 2*precision[i]*recall[i]/(precision[i]+recall[i])\n",
    "            print('Train F1: {:.6f}'.format(\n",
    "          sum(F1)/num_class,\n",
    "      ))\n",
    "            F1_list_train.append(sum(F1)/num_class)\n",
    "\n",
    "        model.eval()\n",
    "        print('epoch: {}, loss: {:.4}'.format(epoch2, loss.data.item()))\n",
    "        train_loss_list.append(loss.data.item())\n",
    "\n",
    "      # model.eval()\n",
    "        num_TP = [0]*num_class\n",
    "        num_FP = [0]*num_class\n",
    "        num_FN = [0]*num_class\n",
    "        num_TN = [0]*num_class\n",
    "        eval_loss = 0\n",
    "        eval_acc = 0\n",
    "        eval_TP = [0]*num_class\n",
    "        eval_FP = [0]*num_class\n",
    "        eval_FN = [0]*num_class\n",
    "        eval_TN  = [0]*num_class\n",
    "        precision = [0]*num_class\n",
    "        recall = [0]*num_class\n",
    "        F1 = [0]*num_class\n",
    "        \n",
    "        \n",
    "        # here add roc\n",
    "        FPR = [0]*num_class\n",
    "        TPR = [0]*num_class\n",
    "        y_pred = [0]*num_class\n",
    "        \n",
    "        \n",
    "        \n",
    "        for data in test_loader:\n",
    "            img, label = data\n",
    "          # img = img.view(img.size(0), -1)\n",
    "            if torch.cuda.is_available():\n",
    "                img = img.cuda()\n",
    "                label = label.cuda()\n",
    "\n",
    "            out = model(img)\n",
    "            loss = criterion(out, label)\n",
    "            eval_loss += loss.data.item()*label.size(0)\n",
    "            _, pred = torch.max(out, 1)\n",
    "            num_correct = (pred == label).sum()\n",
    "            y_pred = pred.cpu().numpy()\n",
    "            lable_pred = label.cpu().numpy()\n",
    "            if epoch2 == totEpoch - 1:\n",
    "                y_pred_list.append(y_pred)\n",
    "                lable_list.append(lable_pred)\n",
    "            for i in range(num_class):\n",
    "                num_TP[i] = (((pred == i) & (label == i))).sum()\n",
    "                num_FP[i] = (((pred == i) & (label != i))).sum()\n",
    "                num_FN[i] = (((pred != i) & (label == i))).sum()\n",
    "                num_TN[i] = (((pred != i) & (label != i))).sum()\n",
    "                eval_TP[i] += num_TP[i].item()\n",
    "                eval_FP[i] += num_FP[i].item()\n",
    "                eval_FN[i] += num_FN[i].item()\n",
    "                eval_TN[i] += num_TN[i].item()\n",
    "            eval_acc += num_correct.item()\n",
    "        for i in range(num_class):\n",
    "            if (eval_TP[i] + eval_FP[i]) == 0:\n",
    "                precision[i] = 1\n",
    "            elif (eval_FP[i] + eval_TN[i]) == 0:\n",
    "                FPR[i] = 0\n",
    "            elif (eval_TP[i] + eval_FN[i]) == 0:\n",
    "                TPR[i] = 0\n",
    "            else:\n",
    "                FPR[i] = eval_FP[i] / (eval_FP[i] + eval_TN[i])\n",
    "                TPR[i] = eval_TP[i] / (eval_TP[i] + eval_FN[i])\n",
    "                precision[i] = eval_TP[i] / (eval_TP[i] + eval_FP[i])\n",
    "            if (eval_TP[i] + eval_FN[i]) == 0:\n",
    "                recall[i] = 1\n",
    "            else:\n",
    "                recall[i] = eval_TP[i] / (eval_TP[i] + eval_FN[i])\n",
    "            if (precision[i]+recall[i]) == 0:\n",
    "                F1[i] = 0\n",
    "            else:\n",
    "                F1[i] = 2*precision[i]*recall[i]/(precision[i]+recall[i])\n",
    "        print('Test Loss: {:.6f}, Acc: {:.6f}, Pre: {:.6f}, Rec: {:.6f}, F1: {:.6f}'.format(\n",
    "          eval_loss / (len_test),\n",
    "          eval_acc / (len_test),\n",
    "          sum(precision)/num_class,\n",
    "          sum(recall)/num_class,\n",
    "          sum(F1)/num_class\n",
    "      ))\n",
    "        loss_list.append(loss.data.item())\n",
    "        accuracy_list.append(eval_acc / (len_test))\n",
    "        precision_list.append(sum(precision)/num_class)\n",
    "        recall_list.append(sum(recall)/num_class)\n",
    "        F1_list.append(sum(F1)/num_class)\n",
    "        TPR0_list.append(TPR[1])\n",
    "        FPR0_list.append(FPR[1])\n",
    "    return [np.array(accuracy_list), np.array(precision_list), np.array(recall_list), np.array(F1_list), np.array(F1_list_train), np.array(loss_list), np.array(train_loss_list)], TPR0_list, FPR0_list, y_pred_list, lable_list\n",
    "\n",
    "def train_model(vector_vae, label, model, totEpoch, num_class, random_s):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(vector_vae, label, test_size=0.2, random_state = random_s)\n",
    "    train_loader, test_loader = process_sentiment(X_train, X_test, y_train, y_test)\n",
    "    results= classifier(model, train_loader, test_loader, totEpoch, num_class, len(y_test))\n",
    "    return results\n",
    "\n",
    "def train_cross_val(input_dim, vector_vae, label, totEpoch, num_class, k):\n",
    "    results = [np.array([0.0]*totEpoch),np.array([0.0]*totEpoch),np.array([0.0]*totEpoch),np.array([0.0]*totEpoch),np.array([0.0]*3*totEpoch),np.array([0.0]*totEpoch),np.array([0.0]*totEpoch)]\n",
    "    kf = StratifiedKFold(n_splits=k)\n",
    "    c = 0\n",
    "    var_test, var_train = [], []\n",
    "    F1_train, F1_test = [], []\n",
    "    Loss_train, Loss_test = [], []\n",
    "    for train_index, test_index in kf.split(vector_vae, label):\n",
    "        print('The ', c, ' th fold cross validation:')\n",
    "        X_train = vector_vae[train_index]\n",
    "        y_train = label[train_index]\n",
    "        X_test = vector_vae[test_index]\n",
    "        y_test = label[test_index]\n",
    "        train_loader, test_loader = process_sentiment(X_train, X_test, y_train, y_test)\n",
    "        result_list, TPR, FPR, y_pred, lable_list= classifier(input_dim, train_loader, test_loader, totEpoch, num_class, len(y_test))\n",
    "        var_test.append(result_list[3])\n",
    "        var_train.append(result_list[4])\n",
    "        # print((var_train))\n",
    "        F1_train.append(result_list[4])\n",
    "        # print((F1_train))\n",
    "        F1_test.append(result_list[3])\n",
    "        Loss_train.append(result_list[6])\n",
    "        Loss_test.append(result_list[5])\n",
    "        for i in range(7):\n",
    "            # print(i)\n",
    "            results[i] += result_list[i]\n",
    "        c += 1\n",
    "    for i in range(7):\n",
    "        results[i] /= k\n",
    "    var_test = np.array(var_test)\n",
    "    var_train = np.array(var_train)\n",
    "    results.append(np.var(var_train, axis = 0))\n",
    "    results.append(np.var(var_test, axis = 0))\n",
    "    return results, F1_train, F1_test, Loss_train, Loss_test, TPR, FPR, y_pred, lable_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c9c6a4c-b6df-4a5b-9421-989c010785ea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GN7G8skgFUR8",
    "outputId": "0fa55c5a-456d-4daf-bd8f-22c45fe58fed",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The  0  th fold cross validation:\n",
      "Train F1: 0.000000\n",
      "Train F1: 0.039371\n",
      "Train F1: 0.050415\n",
      "epoch: 0, loss: 1.549\n",
      "Test Loss: 1.334253, Acc: 0.330208, Pre: 0.776736, Rec: 0.666667, F1: 0.498825\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.436330\n",
      "Train F1: 0.569575\n",
      "epoch: 1, loss: 1.125\n",
      "Test Loss: 1.123410, Acc: 0.350228, Pre: 0.543297, Rec: 0.473489, F1: 0.475443\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.455379\n",
      "Train F1: 0.620359\n",
      "epoch: 2, loss: 1.076\n",
      "Test Loss: 1.107346, Acc: 0.319311, Pre: 0.548793, Rec: 0.575127, F1: 0.545873\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.533377\n",
      "Train F1: 0.699266\n",
      "epoch: 3, loss: 0.9154\n",
      "Test Loss: 1.185470, Acc: 0.326153, Pre: 0.558259, Rec: 0.637028, F1: 0.545327\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.547158\n",
      "Train F1: 0.702579\n",
      "epoch: 4, loss: 0.9247\n",
      "Test Loss: 1.210475, Acc: 0.330208, Pre: 0.562679, Rec: 0.644242, F1: 0.544660\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.550832\n",
      "Train F1: 0.718968\n",
      "epoch: 5, loss: 0.9891\n",
      "Test Loss: 1.187416, Acc: 0.335530, Pre: 0.563960, Rec: 0.648791, F1: 0.549477\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.554051\n",
      "Train F1: 0.725047\n",
      "epoch: 6, loss: 0.8471\n",
      "Test Loss: 1.182788, Acc: 0.340091, Pre: 0.568286, Rec: 0.658489, F1: 0.553738\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.557229\n",
      "Train F1: 0.733432\n",
      "epoch: 7, loss: 0.8161\n",
      "Test Loss: 1.169821, Acc: 0.343639, Pre: 0.568545, Rec: 0.656517, F1: 0.551305\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.559795\n",
      "Train F1: 0.739111\n",
      "epoch: 8, loss: 0.8295\n",
      "Test Loss: 1.147460, Acc: 0.354029, Pre: 0.570994, Rec: 0.658922, F1: 0.561894\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.563951\n",
      "Train F1: 0.747265\n",
      "epoch: 9, loss: 0.7951\n",
      "Test Loss: 1.128942, Acc: 0.384440, Pre: 0.589243, Rec: 0.675854, F1: 0.582782\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.567378\n",
      "Train F1: 0.756597\n",
      "epoch: 10, loss: 0.8557\n",
      "Test Loss: 1.131577, Acc: 0.411809, Pre: 0.600973, Rec: 0.660727, F1: 0.576201\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.570982\n",
      "Train F1: 0.766168\n",
      "epoch: 11, loss: 0.7332\n",
      "Test Loss: 1.104824, Acc: 0.433604, Pre: 0.608633, Rec: 0.674964, F1: 0.619605\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.574708\n",
      "Train F1: 0.772352\n",
      "epoch: 12, loss: 0.7937\n",
      "Test Loss: 1.112852, Acc: 0.426761, Pre: 0.605990, Rec: 0.694357, F1: 0.630353\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.579346\n",
      "Train F1: 0.783134\n",
      "epoch: 13, loss: 0.8446\n",
      "Test Loss: 1.085626, Acc: 0.467562, Pre: 0.634319, Rec: 0.693262, F1: 0.632335\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.581931\n",
      "Train F1: 0.789613\n",
      "epoch: 14, loss: 0.6188\n",
      "Test Loss: 1.120045, Acc: 0.446782, Pre: 0.614596, Rec: 0.697400, F1: 0.642962\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.586382\n",
      "Train F1: 0.797758\n",
      "epoch: 15, loss: 0.6558\n",
      "Test Loss: 1.076062, Acc: 0.479980, Pre: 0.633684, Rec: 0.684696, F1: 0.644782\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.587345\n",
      "Train F1: 0.801910\n",
      "epoch: 16, loss: 0.6025\n",
      "Test Loss: 1.087022, Acc: 0.480233, Pre: 0.637501, Rec: 0.706199, F1: 0.650081\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.590017\n",
      "Train F1: 0.807171\n",
      "epoch: 17, loss: 0.5973\n",
      "Test Loss: 1.068668, Acc: 0.493158, Pre: 0.641942, Rec: 0.703432, F1: 0.659264\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.594481\n",
      "Train F1: 0.817531\n",
      "epoch: 18, loss: 0.6897\n",
      "Test Loss: 1.054591, Acc: 0.498733, Pre: 0.647419, Rec: 0.711890, F1: 0.669283\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.594432\n",
      "Train F1: 0.816521\n",
      "epoch: 19, loss: 0.7245\n",
      "Test Loss: 1.050080, Acc: 0.501267, Pre: 0.647053, Rec: 0.714624, F1: 0.666749\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.598003\n",
      "Train F1: 0.823400\n",
      "epoch: 20, loss: 0.5804\n",
      "Test Loss: 1.067007, Acc: 0.495185, Pre: 0.645153, Rec: 0.712812, F1: 0.673616\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.599607\n",
      "Train F1: 0.827194\n",
      "epoch: 21, loss: 0.8937\n",
      "Test Loss: 1.056925, Acc: 0.514445, Pre: 0.661986, Rec: 0.714601, F1: 0.686175\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.601202\n",
      "Train F1: 0.830724\n",
      "epoch: 22, loss: 0.704\n",
      "Test Loss: 1.082855, Acc: 0.528130, Pre: 0.664194, Rec: 0.698623, F1: 0.668255\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.603283\n",
      "Train F1: 0.834580\n",
      "epoch: 23, loss: 0.6134\n",
      "Test Loss: 1.072593, Acc: 0.517993, Pre: 0.658075, Rec: 0.723951, F1: 0.683388\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.605737\n",
      "Train F1: 0.839857\n",
      "epoch: 24, loss: 0.5056\n",
      "Test Loss: 1.036667, Acc: 0.532438, Pre: 0.667287, Rec: 0.735102, F1: 0.696665\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.608002\n",
      "Train F1: 0.843168\n",
      "epoch: 25, loss: 0.3438\n",
      "Test Loss: 1.056810, Acc: 0.534465, Pre: 0.672131, Rec: 0.713971, F1: 0.690985\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.610494\n",
      "Train F1: 0.847510\n",
      "epoch: 26, loss: 0.4739\n",
      "Test Loss: 1.051875, Acc: 0.524328, Pre: 0.664394, Rec: 0.731544, F1: 0.693672\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.612825\n",
      "Train F1: 0.849815\n",
      "epoch: 27, loss: 0.6635\n",
      "Test Loss: 1.033822, Acc: 0.554486, Pre: 0.686226, Rec: 0.725332, F1: 0.698520\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.612897\n",
      "Train F1: 0.852673\n",
      "epoch: 28, loss: 0.407\n",
      "Test Loss: 1.023808, Acc: 0.543588, Pre: 0.680545, Rec: 0.736127, F1: 0.706269\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.615577\n",
      "Train F1: 0.858654\n",
      "epoch: 29, loss: 0.7816\n",
      "Test Loss: 1.017898, Acc: 0.557020, Pre: 0.686902, Rec: 0.742717, F1: 0.711685\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.617378\n",
      "Train F1: 0.857779\n",
      "epoch: 30, loss: 0.3552\n",
      "Test Loss: 1.015130, Acc: 0.553979, Pre: 0.693246, Rec: 0.749300, F1: 0.719245\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.616386\n",
      "Train F1: 0.859196\n",
      "epoch: 31, loss: 0.626\n",
      "Test Loss: 1.025914, Acc: 0.565129, Pre: 0.691716, Rec: 0.732511, F1: 0.708358\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.619984\n",
      "Train F1: 0.866050\n",
      "epoch: 32, loss: 0.8603\n",
      "Test Loss: 0.977579, Acc: 0.567410, Pre: 0.700388, Rec: 0.726172, F1: 0.712639\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.622490\n",
      "Train F1: 0.868519\n",
      "epoch: 33, loss: 0.5032\n",
      "Test Loss: 0.975207, Acc: 0.583376, Pre: 0.709800, Rec: 0.742060, F1: 0.723766\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.624158\n",
      "Train F1: 0.873199\n",
      "epoch: 34, loss: 0.6099\n",
      "Test Loss: 0.994420, Acc: 0.580841, Pre: 0.703878, Rec: 0.756568, F1: 0.726643\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.625669\n",
      "Train F1: 0.874505\n",
      "epoch: 35, loss: 0.4472\n",
      "Test Loss: 0.992199, Acc: 0.577800, Pre: 0.707738, Rec: 0.762082, F1: 0.733062\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.627899\n",
      "Train F1: 0.878373\n",
      "epoch: 36, loss: 0.6232\n",
      "Test Loss: 0.994506, Acc: 0.587430, Pre: 0.709967, Rec: 0.750657, F1: 0.728184\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.628456\n",
      "Train F1: 0.879533\n",
      "epoch: 37, loss: 0.5062\n",
      "Test Loss: 1.002526, Acc: 0.577800, Pre: 0.714046, Rec: 0.778446, F1: 0.743368\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.630842\n",
      "Train F1: 0.885856\n",
      "epoch: 38, loss: 0.3893\n",
      "Test Loss: 1.026347, Acc: 0.570198, Pre: 0.711378, Rec: 0.749966, F1: 0.729269\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.630648\n",
      "Train F1: 0.886562\n",
      "epoch: 39, loss: 0.5037\n",
      "Test Loss: 1.025308, Acc: 0.577040, Pre: 0.705461, Rec: 0.775385, F1: 0.737357\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.632376\n",
      "Train F1: 0.889011\n",
      "epoch: 40, loss: 0.6474\n",
      "Test Loss: 1.017491, Acc: 0.591738, Pre: 0.714384, Rec: 0.774412, F1: 0.742011\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.633539\n",
      "Train F1: 0.891450\n",
      "epoch: 41, loss: 0.4486\n",
      "Test Loss: 1.007027, Acc: 0.584389, Pre: 0.711321, Rec: 0.774910, F1: 0.740548\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.634151\n",
      "Train F1: 0.892653\n",
      "epoch: 42, loss: 0.3645\n",
      "Test Loss: 1.007968, Acc: 0.589965, Pre: 0.721123, Rec: 0.770962, F1: 0.744497\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.636106\n",
      "Train F1: 0.897586\n",
      "epoch: 43, loss: 0.3741\n",
      "Test Loss: 1.029901, Acc: 0.595540, Pre: 0.720981, Rec: 0.750866, F1: 0.735074\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.637437\n",
      "Train F1: 0.899267\n",
      "epoch: 44, loss: 0.3642\n",
      "Test Loss: 0.999797, Acc: 0.610492, Pre: 0.726764, Rec: 0.771476, F1: 0.747116\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.637330\n",
      "Train F1: 0.899982\n",
      "epoch: 45, loss: 0.3787\n",
      "Test Loss: 1.056419, Acc: 0.577800, Pre: 0.728919, Rec: 0.761564, F1: 0.740204\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.637929\n",
      "Train F1: 0.902469\n",
      "epoch: 46, loss: 0.5049\n",
      "Test Loss: 1.037604, Acc: 0.617587, Pre: 0.729525, Rec: 0.779845, F1: 0.752513\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.640036\n",
      "Train F1: 0.906249\n",
      "epoch: 47, loss: 0.4542\n",
      "Test Loss: 1.058334, Acc: 0.591232, Pre: 0.727200, Rec: 0.772735, F1: 0.747369\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.638702\n",
      "Train F1: 0.904580\n",
      "epoch: 48, loss: 0.5299\n",
      "Test Loss: 1.110410, Acc: 0.601622, Pre: 0.719396, Rec: 0.766277, F1: 0.741236\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.638620\n",
      "Train F1: 0.905339\n",
      "epoch: 49, loss: 0.4667\n",
      "Test Loss: 1.042023, Acc: 0.610492, Pre: 0.730966, Rec: 0.783259, F1: 0.755461\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.641413\n",
      "Train F1: 0.910136\n",
      "epoch: 50, loss: 0.2884\n",
      "Test Loss: 1.149434, Acc: 0.598074, Pre: 0.714129, Rec: 0.778598, F1: 0.742950\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.643118\n",
      "Train F1: 0.913070\n",
      "epoch: 51, loss: 0.427\n",
      "Test Loss: 1.035310, Acc: 0.615813, Pre: 0.735155, Rec: 0.772538, F1: 0.753014\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.642982\n",
      "Train F1: 0.914288\n",
      "epoch: 52, loss: 0.278\n",
      "Test Loss: 1.076382, Acc: 0.618094, Pre: 0.738135, Rec: 0.776440, F1: 0.756274\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.643496\n",
      "Train F1: 0.916259\n",
      "epoch: 53, loss: 0.2722\n",
      "Test Loss: 1.079080, Acc: 0.615560, Pre: 0.732696, Rec: 0.780584, F1: 0.755054\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.644033\n",
      "Train F1: 0.916513\n",
      "epoch: 54, loss: 0.2885\n",
      "Test Loss: 1.026676, Acc: 0.626457, Pre: 0.743498, Rec: 0.778934, F1: 0.760466\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.645652\n",
      "Train F1: 0.919105\n",
      "epoch: 55, loss: 0.2769\n",
      "Test Loss: 1.090786, Acc: 0.629752, Pre: 0.740292, Rec: 0.781928, F1: 0.759257\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.645294\n",
      "Train F1: 0.918328\n",
      "epoch: 56, loss: 0.3186\n",
      "Test Loss: 1.090037, Acc: 0.624937, Pre: 0.740357, Rec: 0.795278, F1: 0.765908\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.645286\n",
      "Train F1: 0.920307\n",
      "epoch: 57, loss: 0.4476\n",
      "Test Loss: 1.076273, Acc: 0.616827, Pre: 0.747152, Rec: 0.758599, F1: 0.751917\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.645337\n",
      "Train F1: 0.921288\n",
      "epoch: 58, loss: 0.5215\n",
      "Test Loss: 1.138675, Acc: 0.614800, Pre: 0.732965, Rec: 0.782957, F1: 0.756490\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.645443\n",
      "Train F1: 0.921119\n",
      "epoch: 59, loss: 0.3821\n",
      "Test Loss: 1.131125, Acc: 0.639128, Pre: 0.743139, Rec: 0.792189, F1: 0.763948\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.646459\n",
      "Train F1: 0.922469\n",
      "epoch: 60, loss: 0.2185\n",
      "Test Loss: 1.118018, Acc: 0.626457, Pre: 0.747193, Rec: 0.786107, F1: 0.765487\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.647092\n",
      "Train F1: 0.925332\n",
      "epoch: 61, loss: 0.2385\n",
      "Test Loss: 1.099385, Acc: 0.644703, Pre: 0.758669, Rec: 0.774486, F1: 0.766029\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.647564\n",
      "Train F1: 0.926944\n",
      "epoch: 62, loss: 0.2908\n",
      "Test Loss: 1.120090, Acc: 0.630765, Pre: 0.748283, Rec: 0.774589, F1: 0.761032\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.647240\n",
      "Train F1: 0.925919\n",
      "epoch: 63, loss: 0.3615\n",
      "Test Loss: 1.182559, Acc: 0.642169, Pre: 0.747965, Rec: 0.773959, F1: 0.757570\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.647594\n",
      "Train F1: 0.928544\n",
      "epoch: 64, loss: 0.4863\n",
      "Test Loss: 1.102878, Acc: 0.646477, Pre: 0.751287, Rec: 0.797860, F1: 0.772764\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.648743\n",
      "Train F1: 0.930854\n",
      "epoch: 65, loss: 0.2447\n",
      "Test Loss: 1.148559, Acc: 0.647491, Pre: 0.757149, Rec: 0.788120, F1: 0.771941\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.648808\n",
      "Train F1: 0.931093\n",
      "epoch: 66, loss: 0.2015\n",
      "Test Loss: 1.181338, Acc: 0.622149, Pre: 0.746193, Rec: 0.784558, F1: 0.763867\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.649073\n",
      "Train F1: 0.933076\n",
      "epoch: 67, loss: 0.3632\n",
      "Test Loss: 1.105408, Acc: 0.656614, Pre: 0.764873, Rec: 0.785147, F1: 0.774424\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.650359\n",
      "Train F1: 0.935463\n",
      "epoch: 68, loss: 0.09528\n",
      "Test Loss: 1.251391, Acc: 0.637608, Pre: 0.743067, Rec: 0.793626, F1: 0.765513\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.651329\n",
      "Train F1: 0.936160\n",
      "epoch: 69, loss: 0.1631\n",
      "Test Loss: 1.165376, Acc: 0.638115, Pre: 0.748348, Rec: 0.795916, F1: 0.770841\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.651082\n",
      "Train F1: 0.934850\n",
      "epoch: 70, loss: 0.2414\n",
      "Test Loss: 1.262151, Acc: 0.620122, Pre: 0.740269, Rec: 0.794090, F1: 0.764656\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.649056\n",
      "Train F1: 0.932826\n",
      "epoch: 71, loss: 0.2519\n",
      "Test Loss: 1.169304, Acc: 0.652306, Pre: 0.758020, Rec: 0.799637, F1: 0.777263\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.651376\n",
      "Train F1: 0.937687\n",
      "epoch: 72, loss: 0.1796\n",
      "Test Loss: 1.271450, Acc: 0.634313, Pre: 0.739537, Rec: 0.802307, F1: 0.768451\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.651436\n",
      "Train F1: 0.938743\n",
      "epoch: 73, loss: 0.1739\n",
      "Test Loss: 1.199040, Acc: 0.655094, Pre: 0.763823, Rec: 0.769039, F1: 0.764766\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.651828\n",
      "Train F1: 0.940027\n",
      "epoch: 74, loss: 0.2958\n",
      "Test Loss: 1.284378, Acc: 0.656868, Pre: 0.755060, Rec: 0.798803, F1: 0.773129\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.650255\n",
      "Train F1: 0.938139\n",
      "epoch: 75, loss: 0.4035\n",
      "Test Loss: 1.228637, Acc: 0.655854, Pre: 0.756420, Rec: 0.805716, F1: 0.779186\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.650566\n",
      "Train F1: 0.937842\n",
      "epoch: 76, loss: 0.2715\n",
      "Test Loss: 1.276145, Acc: 0.633553, Pre: 0.741544, Rec: 0.800214, F1: 0.768911\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.651578\n",
      "Train F1: 0.939350\n",
      "epoch: 77, loss: 0.3821\n",
      "Test Loss: 1.263457, Acc: 0.642930, Pre: 0.756610, Rec: 0.794777, F1: 0.774835\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.651442\n",
      "Train F1: 0.940067\n",
      "epoch: 78, loss: 0.3364\n",
      "Test Loss: 1.253393, Acc: 0.665231, Pre: 0.762783, Rec: 0.801301, F1: 0.779184\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.651341\n",
      "Train F1: 0.940707\n",
      "epoch: 79, loss: 0.2606\n",
      "Test Loss: 1.301336, Acc: 0.642423, Pre: 0.748484, Rec: 0.793725, F1: 0.769704\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.650809\n",
      "Train F1: 0.938881\n",
      "epoch: 80, loss: 0.4035\n",
      "Test Loss: 1.267753, Acc: 0.650279, Pre: 0.759690, Rec: 0.783250, F1: 0.771057\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.653460\n",
      "Train F1: 0.944741\n",
      "epoch: 81, loss: 0.4096\n",
      "Test Loss: 1.267117, Acc: 0.641156, Pre: 0.754314, Rec: 0.787739, F1: 0.770384\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.654869\n",
      "Train F1: 0.947665\n",
      "epoch: 82, loss: 0.3579\n",
      "Test Loss: 1.316474, Acc: 0.630258, Pre: 0.751942, Rec: 0.784699, F1: 0.766637\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.653029\n",
      "Train F1: 0.943924\n",
      "epoch: 83, loss: 0.4952\n",
      "Test Loss: 1.280650, Acc: 0.648251, Pre: 0.768362, Rec: 0.765124, F1: 0.766734\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.653283\n",
      "Train F1: 0.945190\n",
      "epoch: 84, loss: 0.1517\n",
      "Test Loss: 1.405124, Acc: 0.647745, Pre: 0.748979, Rec: 0.809731, F1: 0.776626\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.653955\n",
      "Train F1: 0.947109\n",
      "epoch: 85, loss: 0.3945\n",
      "Test Loss: 1.305718, Acc: 0.642676, Pre: 0.759178, Rec: 0.773483, F1: 0.766126\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.651309\n",
      "Train F1: 0.942840\n",
      "epoch: 86, loss: 0.2788\n",
      "Test Loss: 1.321215, Acc: 0.664977, Pre: 0.765249, Rec: 0.807880, F1: 0.785468\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.653834\n",
      "Train F1: 0.947986\n",
      "epoch: 87, loss: 0.1543\n",
      "Test Loss: 1.345784, Acc: 0.645210, Pre: 0.756299, Rec: 0.788693, F1: 0.771825\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.654567\n",
      "Train F1: 0.948515\n",
      "epoch: 88, loss: 0.1121\n",
      "Test Loss: 1.385252, Acc: 0.658388, Pre: 0.757155, Rec: 0.800130, F1: 0.776859\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.657168\n",
      "Train F1: 0.954218\n",
      "epoch: 89, loss: 0.2486\n",
      "Test Loss: 1.335065, Acc: 0.661429, Pre: 0.770137, Rec: 0.773412, F1: 0.771457\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.655002\n",
      "Train F1: 0.948374\n",
      "epoch: 90, loss: 0.149\n",
      "Test Loss: 1.417505, Acc: 0.663457, Pre: 0.760074, Rec: 0.808957, F1: 0.780412\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.654262\n",
      "Train F1: 0.950758\n",
      "epoch: 91, loss: 0.3086\n",
      "Test Loss: 1.383133, Acc: 0.653066, Pre: 0.760813, Rec: 0.791288, F1: 0.775520\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.654295\n",
      "Train F1: 0.948896\n",
      "epoch: 92, loss: 0.3469\n",
      "Test Loss: 1.469776, Acc: 0.669539, Pre: 0.766816, Rec: 0.792058, F1: 0.774425\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.653141\n",
      "Train F1: 0.947967\n",
      "epoch: 93, loss: 0.1828\n",
      "Test Loss: 1.432809, Acc: 0.659149, Pre: 0.757537, Rec: 0.801598, F1: 0.776155\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.653354\n",
      "Train F1: 0.948875\n",
      "epoch: 94, loss: 0.2024\n",
      "Test Loss: 1.398531, Acc: 0.670552, Pre: 0.771338, Rec: 0.775100, F1: 0.771373\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.656115\n",
      "Train F1: 0.952956\n",
      "epoch: 95, loss: 0.2473\n",
      "Test Loss: 1.453112, Acc: 0.664217, Pre: 0.762456, Rec: 0.804730, F1: 0.781570\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.655120\n",
      "Train F1: 0.953274\n",
      "epoch: 96, loss: 0.09891\n",
      "Test Loss: 1.429785, Acc: 0.640395, Pre: 0.761183, Rec: 0.792131, F1: 0.775392\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.655996\n",
      "Train F1: 0.953804\n",
      "epoch: 97, loss: 0.1574\n",
      "Test Loss: 1.528250, Acc: 0.654840, Pre: 0.754442, Rec: 0.809935, F1: 0.780240\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.655341\n",
      "Train F1: 0.952801\n",
      "epoch: 98, loss: 0.2811\n",
      "Test Loss: 1.518581, Acc: 0.651546, Pre: 0.758331, Rec: 0.808303, F1: 0.781908\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.655037\n",
      "Train F1: 0.954203\n",
      "epoch: 99, loss: 0.1956\n",
      "Test Loss: 1.490828, Acc: 0.674861, Pre: 0.770147, Rec: 0.795584, F1: 0.779029\n",
      "The  1  th fold cross validation:\n",
      "Train F1: 0.000000\n",
      "Train F1: 0.038568\n",
      "Train F1: 0.048549\n",
      "epoch: 0, loss: 1.399\n",
      "Test Loss: 1.318903, Acc: 0.330715, Pre: 0.643458, Rec: 0.666909, F1: 0.500326\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.408320\n",
      "Train F1: 0.567142\n",
      "epoch: 1, loss: 1.103\n",
      "Test Loss: 1.109778, Acc: 0.360618, Pre: 0.602056, Rec: 0.626938, F1: 0.584370\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.486726\n",
      "Train F1: 0.668567\n",
      "epoch: 2, loss: 1.078\n",
      "Test Loss: 1.097851, Acc: 0.314242, Pre: 0.587469, Rec: 0.483189, F1: 0.518399\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.502508\n",
      "Train F1: 0.686452\n",
      "epoch: 3, loss: 0.9301\n",
      "Test Loss: 1.131952, Acc: 0.347694, Pre: 0.555718, Rec: 0.623403, F1: 0.584119\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.519152\n",
      "Train F1: 0.704139\n",
      "epoch: 4, loss: 0.8562\n",
      "Test Loss: 1.085325, Acc: 0.405727, Pre: 0.575158, Rec: 0.570975, F1: 0.569989\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.523617\n",
      "Train F1: 0.704369\n",
      "epoch: 5, loss: 0.9957\n",
      "Test Loss: 1.019094, Acc: 0.478206, Pre: 0.608962, Rec: 0.593699, F1: 0.592219\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.532061\n",
      "Train F1: 0.718167\n",
      "epoch: 6, loss: 1.011\n",
      "Test Loss: 1.017456, Acc: 0.467816, Pre: 0.599283, Rec: 0.600350, F1: 0.591424\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.539833\n",
      "Train F1: 0.730909\n",
      "epoch: 7, loss: 0.8455\n",
      "Test Loss: 0.982527, Acc: 0.508109, Pre: 0.623945, Rec: 0.635440, F1: 0.618582\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.545598\n",
      "Train F1: 0.741110\n",
      "epoch: 8, loss: 0.7781\n",
      "Test Loss: 0.993186, Acc: 0.515205, Pre: 0.623072, Rec: 0.621959, F1: 0.606792\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.552953\n",
      "Train F1: 0.751471\n",
      "epoch: 9, loss: 0.9013\n",
      "Test Loss: 0.989883, Acc: 0.500507, Pre: 0.648155, Rec: 0.628034, F1: 0.633296\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.558291\n",
      "Train F1: 0.760006\n",
      "epoch: 10, loss: 0.9669\n",
      "Test Loss: 0.929783, Acc: 0.552458, Pre: 0.669836, Rec: 0.645272, F1: 0.643315\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.563387\n",
      "Train F1: 0.768362\n",
      "epoch: 11, loss: 0.7417\n",
      "Test Loss: 1.009111, Acc: 0.519513, Pre: 0.653351, Rec: 0.624056, F1: 0.625638\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.570546\n",
      "Train F1: 0.779382\n",
      "epoch: 12, loss: 0.9592\n",
      "Test Loss: 0.915141, Acc: 0.564369, Pre: 0.695173, Rec: 0.661811, F1: 0.668085\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.573774\n",
      "Train F1: 0.784350\n",
      "epoch: 13, loss: 0.6404\n",
      "Test Loss: 0.913958, Acc: 0.550938, Pre: 0.696082, Rec: 0.684990, F1: 0.689609\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.576916\n",
      "Train F1: 0.791457\n",
      "epoch: 14, loss: 0.7485\n",
      "Test Loss: 0.943998, Acc: 0.552965, Pre: 0.691508, Rec: 0.654852, F1: 0.661590\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.582549\n",
      "Train F1: 0.798119\n",
      "epoch: 15, loss: 0.5851\n",
      "Test Loss: 0.950316, Acc: 0.549924, Pre: 0.698331, Rec: 0.661475, F1: 0.672617\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.585244\n",
      "Train F1: 0.804526\n",
      "epoch: 16, loss: 0.8274\n",
      "Test Loss: 0.925530, Acc: 0.561581, Pre: 0.709071, Rec: 0.664974, F1: 0.680034\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.587443\n",
      "Train F1: 0.807596\n",
      "epoch: 17, loss: 0.725\n",
      "Test Loss: 0.881320, Acc: 0.594019, Pre: 0.712879, Rec: 0.691599, F1: 0.691273\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.592228\n",
      "Train F1: 0.815611\n",
      "epoch: 18, loss: 0.7381\n",
      "Test Loss: 0.902372, Acc: 0.587684, Pre: 0.717623, Rec: 0.668096, F1: 0.675867\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.594765\n",
      "Train F1: 0.819699\n",
      "epoch: 19, loss: 0.7506\n",
      "Test Loss: 0.854241, Acc: 0.597821, Pre: 0.722488, Rec: 0.728718, F1: 0.723963\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.598680\n",
      "Train F1: 0.825149\n",
      "epoch: 20, loss: 0.6693\n",
      "Test Loss: 0.935373, Acc: 0.583122, Pre: 0.712851, Rec: 0.697818, F1: 0.692595\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.603847\n",
      "Train F1: 0.832285\n",
      "epoch: 21, loss: 0.4908\n",
      "Test Loss: 0.953568, Acc: 0.572225, Pre: 0.704020, Rec: 0.685879, F1: 0.684370\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.603011\n",
      "Train F1: 0.831915\n",
      "epoch: 22, loss: 0.642\n",
      "Test Loss: 0.849339, Acc: 0.623923, Pre: 0.747064, Rec: 0.695504, F1: 0.705238\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.609568\n",
      "Train F1: 0.842324\n",
      "epoch: 23, loss: 0.6107\n",
      "Test Loss: 0.880200, Acc: 0.597567, Pre: 0.733546, Rec: 0.700818, F1: 0.710805\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.610072\n",
      "Train F1: 0.843406\n",
      "epoch: 24, loss: 0.6066\n",
      "Test Loss: 0.879797, Acc: 0.609478, Pre: 0.730224, Rec: 0.714252, F1: 0.713523\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.611936\n",
      "Train F1: 0.846561\n",
      "epoch: 25, loss: 0.6225\n",
      "Test Loss: 0.874905, Acc: 0.610998, Pre: 0.733721, Rec: 0.702811, F1: 0.707497\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.613244\n",
      "Train F1: 0.848160\n",
      "epoch: 26, loss: 0.5288\n",
      "Test Loss: 0.914012, Acc: 0.584389, Pre: 0.723599, Rec: 0.727753, F1: 0.724258\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.617377\n",
      "Train F1: 0.855919\n",
      "epoch: 27, loss: 0.6448\n",
      "Test Loss: 0.898442, Acc: 0.594019, Pre: 0.731932, Rec: 0.712664, F1: 0.719751\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.619145\n",
      "Train F1: 0.860131\n",
      "epoch: 28, loss: 0.6997\n",
      "Test Loss: 0.848504, Acc: 0.634567, Pre: 0.749650, Rec: 0.725736, F1: 0.726782\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.619278\n",
      "Train F1: 0.859860\n",
      "epoch: 29, loss: 0.5306\n",
      "Test Loss: 0.839434, Acc: 0.635834, Pre: 0.742652, Rec: 0.740773, F1: 0.733576\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.622261\n",
      "Train F1: 0.865405\n",
      "epoch: 30, loss: 0.4651\n",
      "Test Loss: 0.891689, Acc: 0.626204, Pre: 0.738666, Rec: 0.727200, F1: 0.722655\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.622751\n",
      "Train F1: 0.866328\n",
      "epoch: 31, loss: 0.4715\n",
      "Test Loss: 0.844821, Acc: 0.626457, Pre: 0.764837, Rec: 0.726726, F1: 0.742388\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.623882\n",
      "Train F1: 0.868508\n",
      "epoch: 32, loss: 0.4626\n",
      "Test Loss: 0.905980, Acc: 0.598074, Pre: 0.734739, Rec: 0.750453, F1: 0.742030\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.625229\n",
      "Train F1: 0.872400\n",
      "epoch: 33, loss: 0.5465\n",
      "Test Loss: 0.825551, Acc: 0.645210, Pre: 0.748755, Rec: 0.771856, F1: 0.755968\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.627066\n",
      "Train F1: 0.874434\n",
      "epoch: 34, loss: 0.3125\n",
      "Test Loss: 0.843792, Acc: 0.642169, Pre: 0.744462, Rec: 0.752726, F1: 0.741563\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.629533\n",
      "Train F1: 0.879262\n",
      "epoch: 35, loss: 0.3238\n",
      "Test Loss: 0.929233, Acc: 0.608464, Pre: 0.738047, Rec: 0.727403, F1: 0.727086\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.631084\n",
      "Train F1: 0.881092\n",
      "epoch: 36, loss: 0.5718\n",
      "Test Loss: 0.851915, Acc: 0.652813, Pre: 0.776625, Rec: 0.726331, F1: 0.739079\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.632894\n",
      "Train F1: 0.885741\n",
      "epoch: 37, loss: 0.4868\n",
      "Test Loss: 0.877300, Acc: 0.642930, Pre: 0.745661, Rec: 0.761509, F1: 0.747916\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.632169\n",
      "Train F1: 0.886171\n",
      "epoch: 38, loss: 0.5265\n",
      "Test Loss: 0.829617, Acc: 0.651546, Pre: 0.766561, Rec: 0.759176, F1: 0.760229\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.635008\n",
      "Train F1: 0.888667\n",
      "epoch: 39, loss: 0.3646\n",
      "Test Loss: 0.891597, Acc: 0.625443, Pre: 0.753673, Rec: 0.740138, F1: 0.743054\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.635021\n",
      "Train F1: 0.890846\n",
      "epoch: 40, loss: 0.5208\n",
      "Test Loss: 0.862967, Acc: 0.643436, Pre: 0.749136, Rec: 0.777612, F1: 0.759879\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.635172\n",
      "Train F1: 0.891507\n",
      "epoch: 41, loss: 0.5982\n",
      "Test Loss: 0.880574, Acc: 0.643943, Pre: 0.771552, Rec: 0.737824, F1: 0.749440\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.636028\n",
      "Train F1: 0.892740\n",
      "epoch: 42, loss: 0.4175\n",
      "Test Loss: 0.843712, Acc: 0.656614, Pre: 0.777944, Rec: 0.757395, F1: 0.765440\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.638558\n",
      "Train F1: 0.898495\n",
      "epoch: 43, loss: 0.3592\n",
      "Test Loss: 0.853734, Acc: 0.662190, Pre: 0.775602, Rec: 0.758979, F1: 0.760817\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.640366\n",
      "Train F1: 0.902475\n",
      "epoch: 44, loss: 0.2886\n",
      "Test Loss: 0.893285, Acc: 0.642423, Pre: 0.765816, Rec: 0.772142, F1: 0.767520\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.640775\n",
      "Train F1: 0.901320\n",
      "epoch: 45, loss: 0.2708\n",
      "Test Loss: 0.880583, Acc: 0.664217, Pre: 0.770952, Rec: 0.762831, F1: 0.758423\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.639663\n",
      "Train F1: 0.901633\n",
      "epoch: 46, loss: 0.3817\n",
      "Test Loss: 0.851837, Acc: 0.667511, Pre: 0.771800, Rec: 0.780613, F1: 0.773190\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.639925\n",
      "Train F1: 0.902656\n",
      "epoch: 47, loss: 0.4889\n",
      "Test Loss: 0.866376, Acc: 0.655347, Pre: 0.769656, Rec: 0.780757, F1: 0.773600\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.642349\n",
      "Train F1: 0.906646\n",
      "epoch: 48, loss: 0.4295\n",
      "Test Loss: 0.914036, Acc: 0.649012, Pre: 0.766297, Rec: 0.773817, F1: 0.767390\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.640722\n",
      "Train F1: 0.905853\n",
      "epoch: 49, loss: 0.3591\n",
      "Test Loss: 0.905838, Acc: 0.650279, Pre: 0.775586, Rec: 0.752693, F1: 0.761446\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.643992\n",
      "Train F1: 0.910918\n",
      "epoch: 50, loss: 0.3255\n",
      "Test Loss: 0.943100, Acc: 0.632032, Pre: 0.771136, Rec: 0.753512, F1: 0.761682\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.641876\n",
      "Train F1: 0.908543\n",
      "epoch: 51, loss: 0.2818\n",
      "Test Loss: 0.890548, Acc: 0.657121, Pre: 0.774268, Rec: 0.771997, F1: 0.770864\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.642549\n",
      "Train F1: 0.911081\n",
      "epoch: 52, loss: 0.3556\n",
      "Test Loss: 0.949096, Acc: 0.663710, Pre: 0.766241, Rec: 0.775779, F1: 0.763648\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.644545\n",
      "Train F1: 0.914400\n",
      "epoch: 53, loss: 0.3894\n",
      "Test Loss: 0.915864, Acc: 0.657375, Pre: 0.770873, Rec: 0.778616, F1: 0.771963\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.644562\n",
      "Train F1: 0.914700\n",
      "epoch: 54, loss: 0.2339\n",
      "Test Loss: 0.921220, Acc: 0.655094, Pre: 0.772362, Rec: 0.772993, F1: 0.769792\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.644832\n",
      "Train F1: 0.917129\n",
      "epoch: 55, loss: 0.58\n",
      "Test Loss: 0.929569, Acc: 0.662190, Pre: 0.772264, Rec: 0.772786, F1: 0.768916\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.644045\n",
      "Train F1: 0.913510\n",
      "epoch: 56, loss: 0.4619\n",
      "Test Loss: 0.984357, Acc: 0.656614, Pre: 0.767681, Rec: 0.774902, F1: 0.767248\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.644499\n",
      "Train F1: 0.916205\n",
      "epoch: 57, loss: 0.3834\n",
      "Test Loss: 0.943383, Acc: 0.675874, Pre: 0.776563, Rec: 0.784033, F1: 0.774866\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.646355\n",
      "Train F1: 0.920743\n",
      "epoch: 58, loss: 0.3175\n",
      "Test Loss: 0.980978, Acc: 0.664217, Pre: 0.767755, Rec: 0.776251, F1: 0.766283\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.647074\n",
      "Train F1: 0.920189\n",
      "epoch: 59, loss: 0.2935\n",
      "Test Loss: 0.956782, Acc: 0.658388, Pre: 0.772395, Rec: 0.783438, F1: 0.775393\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.646884\n",
      "Train F1: 0.920731\n",
      "epoch: 60, loss: 0.3714\n",
      "Test Loss: 0.942743, Acc: 0.683984, Pre: 0.778646, Rec: 0.796148, F1: 0.782368\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.647163\n",
      "Train F1: 0.922491\n",
      "epoch: 61, loss: 0.1912\n",
      "Test Loss: 1.056338, Acc: 0.626964, Pre: 0.766237, Rec: 0.768091, F1: 0.766413\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.648385\n",
      "Train F1: 0.922929\n",
      "epoch: 62, loss: 0.4099\n",
      "Test Loss: 0.986874, Acc: 0.673594, Pre: 0.781924, Rec: 0.781045, F1: 0.775549\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.648099\n",
      "Train F1: 0.925392\n",
      "epoch: 63, loss: 0.2357\n",
      "Test Loss: 0.998770, Acc: 0.659909, Pre: 0.784150, Rec: 0.763897, F1: 0.770521\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.648953\n",
      "Train F1: 0.924913\n",
      "epoch: 64, loss: 0.4042\n",
      "Test Loss: 1.043547, Acc: 0.654334, Pre: 0.765696, Rec: 0.775700, F1: 0.765834\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.650227\n",
      "Train F1: 0.929219\n",
      "epoch: 65, loss: 0.3016\n",
      "Test Loss: 1.052529, Acc: 0.634060, Pre: 0.763633, Rec: 0.768572, F1: 0.764925\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.649548\n",
      "Train F1: 0.927165\n",
      "epoch: 66, loss: 0.3083\n",
      "Test Loss: 0.938262, Acc: 0.684997, Pre: 0.790740, Rec: 0.800254, F1: 0.793535\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.648440\n",
      "Train F1: 0.925836\n",
      "epoch: 67, loss: 0.1912\n",
      "Test Loss: 0.964244, Acc: 0.673594, Pre: 0.776206, Rec: 0.804327, F1: 0.788279\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.648807\n",
      "Train F1: 0.927664\n",
      "epoch: 68, loss: 0.2091\n",
      "Test Loss: 1.014310, Acc: 0.679169, Pre: 0.779313, Rec: 0.792618, F1: 0.780937\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.649579\n",
      "Train F1: 0.929569\n",
      "epoch: 69, loss: 0.3727\n",
      "Test Loss: 1.063979, Acc: 0.659402, Pre: 0.767537, Rec: 0.781795, F1: 0.770123\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.650721\n",
      "Train F1: 0.930173\n",
      "epoch: 70, loss: 0.3716\n",
      "Test Loss: 0.965445, Acc: 0.687532, Pre: 0.798917, Rec: 0.783751, F1: 0.785902\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.651261\n",
      "Train F1: 0.933183\n",
      "epoch: 71, loss: 0.2578\n",
      "Test Loss: 0.973130, Acc: 0.671313, Pre: 0.777282, Rec: 0.795980, F1: 0.784766\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.649462\n",
      "Train F1: 0.929812\n",
      "epoch: 72, loss: 0.2768\n",
      "Test Loss: 1.006243, Acc: 0.668525, Pre: 0.787315, Rec: 0.768029, F1: 0.775403\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.649352\n",
      "Train F1: 0.928874\n",
      "epoch: 73, loss: 0.374\n",
      "Test Loss: 0.955434, Acc: 0.686771, Pre: 0.784764, Rec: 0.805865, F1: 0.792962\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.653486\n",
      "Train F1: 0.936365\n",
      "epoch: 74, loss: 0.193\n",
      "Test Loss: 1.053691, Acc: 0.663203, Pre: 0.781380, Rec: 0.772812, F1: 0.773607\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.651824\n",
      "Train F1: 0.934697\n",
      "epoch: 75, loss: 0.2621\n",
      "Test Loss: 1.031477, Acc: 0.673087, Pre: 0.782442, Rec: 0.780415, F1: 0.777881\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.650763\n",
      "Train F1: 0.932617\n",
      "epoch: 76, loss: 0.1686\n",
      "Test Loss: 1.073089, Acc: 0.684744, Pre: 0.785594, Rec: 0.777322, F1: 0.771194\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.650125\n",
      "Train F1: 0.932067\n",
      "epoch: 77, loss: 0.1849\n",
      "Test Loss: 0.995572, Acc: 0.691840, Pre: 0.793619, Rec: 0.795671, F1: 0.792355\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.651267\n",
      "Train F1: 0.935486\n",
      "epoch: 78, loss: 0.1745\n",
      "Test Loss: 1.016570, Acc: 0.693867, Pre: 0.785913, Rec: 0.798893, F1: 0.787707\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.650990\n",
      "Train F1: 0.934494\n",
      "epoch: 79, loss: 0.2059\n",
      "Test Loss: 1.042397, Acc: 0.676635, Pre: 0.784288, Rec: 0.792252, F1: 0.785497\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.652507\n",
      "Train F1: 0.938129\n",
      "epoch: 80, loss: 0.2518\n",
      "Test Loss: 1.039419, Acc: 0.667258, Pre: 0.780267, Rec: 0.795847, F1: 0.787322\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.653437\n",
      "Train F1: 0.938610\n",
      "epoch: 81, loss: 0.4556\n",
      "Test Loss: 1.024107, Acc: 0.710340, Pre: 0.805316, Rec: 0.788626, F1: 0.787563\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.652461\n",
      "Train F1: 0.938120\n",
      "epoch: 82, loss: 0.3026\n",
      "Test Loss: 1.023402, Acc: 0.692093, Pre: 0.791345, Rec: 0.803595, F1: 0.794673\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.655135\n",
      "Train F1: 0.942425\n",
      "epoch: 83, loss: 0.3875\n",
      "Test Loss: 1.086245, Acc: 0.675367, Pre: 0.776953, Rec: 0.790471, F1: 0.780926\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.653980\n",
      "Train F1: 0.940972\n",
      "epoch: 84, loss: 0.241\n",
      "Test Loss: 1.049806, Acc: 0.676381, Pre: 0.796762, Rec: 0.766633, F1: 0.777683\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.654586\n",
      "Train F1: 0.942739\n",
      "epoch: 85, loss: 0.2905\n",
      "Test Loss: 1.056101, Acc: 0.677141, Pre: 0.799478, Rec: 0.775209, F1: 0.784536\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.651986\n",
      "Train F1: 0.937600\n",
      "epoch: 86, loss: 0.3009\n",
      "Test Loss: 1.067063, Acc: 0.679422, Pre: 0.783903, Rec: 0.805609, F1: 0.792678\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.653074\n",
      "Train F1: 0.939885\n",
      "epoch: 87, loss: 0.2888\n",
      "Test Loss: 1.094016, Acc: 0.681703, Pre: 0.789250, Rec: 0.781869, F1: 0.782129\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.653305\n",
      "Train F1: 0.942252\n",
      "epoch: 88, loss: 0.2328\n",
      "Test Loss: 1.066800, Acc: 0.680182, Pre: 0.784666, Rec: 0.797734, F1: 0.789164\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.652951\n",
      "Train F1: 0.940754\n",
      "epoch: 89, loss: 0.2822\n",
      "Test Loss: 1.112013, Acc: 0.656614, Pre: 0.777039, Rec: 0.798539, F1: 0.787439\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.652737\n",
      "Train F1: 0.941319\n",
      "epoch: 90, loss: 0.3143\n",
      "Test Loss: 1.077867, Acc: 0.677395, Pre: 0.784221, Rec: 0.796544, F1: 0.787729\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.653526\n",
      "Train F1: 0.941655\n",
      "epoch: 91, loss: 0.274\n",
      "Test Loss: 1.072294, Acc: 0.692093, Pre: 0.793982, Rec: 0.788920, F1: 0.787939\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.652718\n",
      "Train F1: 0.939963\n",
      "epoch: 92, loss: 0.2567\n",
      "Test Loss: 1.108097, Acc: 0.688292, Pre: 0.784114, Rec: 0.807214, F1: 0.792342\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.654674\n",
      "Train F1: 0.943488\n",
      "epoch: 93, loss: 0.1531\n",
      "Test Loss: 1.113608, Acc: 0.679929, Pre: 0.778714, Rec: 0.797346, F1: 0.784766\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.653674\n",
      "Train F1: 0.944053\n",
      "epoch: 94, loss: 0.2931\n",
      "Test Loss: 1.128740, Acc: 0.687025, Pre: 0.795490, Rec: 0.791763, F1: 0.789903\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.655076\n",
      "Train F1: 0.945364\n",
      "epoch: 95, loss: 0.2177\n",
      "Test Loss: 1.099920, Acc: 0.687278, Pre: 0.784381, Rec: 0.808721, F1: 0.793439\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.656985\n",
      "Train F1: 0.949523\n",
      "epoch: 96, loss: 0.3283\n",
      "Test Loss: 1.099664, Acc: 0.704004, Pre: 0.806119, Rec: 0.789771, F1: 0.791860\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.654439\n",
      "Train F1: 0.945264\n",
      "epoch: 97, loss: 0.2644\n",
      "Test Loss: 1.196297, Acc: 0.674861, Pre: 0.783668, Rec: 0.779441, F1: 0.775859\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.652700\n",
      "Train F1: 0.942513\n",
      "epoch: 98, loss: 0.3825\n",
      "Test Loss: 1.095589, Acc: 0.703244, Pre: 0.796560, Rec: 0.796893, F1: 0.790853\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.656498\n",
      "Train F1: 0.948666\n",
      "epoch: 99, loss: 0.1235\n",
      "Test Loss: 1.100627, Acc: 0.705271, Pre: 0.795892, Rec: 0.809232, F1: 0.798075\n",
      "The  2  th fold cross validation:\n",
      "Train F1: 0.000000\n",
      "Train F1: 0.037279\n",
      "Train F1: 0.055952\n",
      "epoch: 0, loss: 1.352\n",
      "Test Loss: 1.329036, Acc: 0.385454, Pre: 0.598287, Rec: 0.717688, F1: 0.631699\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.343964\n",
      "Train F1: 0.511065\n",
      "epoch: 1, loss: 1.11\n",
      "Test Loss: 1.112619, Acc: 0.358591, Pre: 0.661958, Rec: 0.614669, F1: 0.500095\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.440124\n",
      "Train F1: 0.598316\n",
      "epoch: 2, loss: 1.072\n",
      "Test Loss: 1.091177, Acc: 0.460973, Pre: 0.635332, Rec: 0.697016, F1: 0.660963\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.485693\n",
      "Train F1: 0.642072\n",
      "epoch: 3, loss: 1.027\n",
      "Test Loss: 1.056483, Acc: 0.475925, Pre: 0.667368, Rec: 0.652454, F1: 0.652071\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.513426\n",
      "Train F1: 0.657518\n",
      "epoch: 4, loss: 0.938\n",
      "Test Loss: 0.998168, Acc: 0.496959, Pre: 0.672553, Rec: 0.688000, F1: 0.671877\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.524234\n",
      "Train F1: 0.676770\n",
      "epoch: 5, loss: 0.8787\n",
      "Test Loss: 0.985044, Acc: 0.496199, Pre: 0.669968, Rec: 0.663827, F1: 0.665139\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.530791\n",
      "Train F1: 0.690889\n",
      "epoch: 6, loss: 0.8204\n",
      "Test Loss: 0.965604, Acc: 0.519007, Pre: 0.682324, Rec: 0.703197, F1: 0.683722\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.536932\n",
      "Train F1: 0.702530\n",
      "epoch: 7, loss: 0.8496\n",
      "Test Loss: 0.935899, Acc: 0.539534, Pre: 0.694965, Rec: 0.691297, F1: 0.693109\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.543543\n",
      "Train F1: 0.717753\n",
      "epoch: 8, loss: 0.9528\n",
      "Test Loss: 0.947022, Acc: 0.541561, Pre: 0.692611, Rec: 0.689647, F1: 0.691088\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.549625\n",
      "Train F1: 0.729084\n",
      "epoch: 9, loss: 0.8744\n",
      "Test Loss: 0.927189, Acc: 0.556766, Pre: 0.703861, Rec: 0.703630, F1: 0.703686\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.554890\n",
      "Train F1: 0.739074\n",
      "epoch: 10, loss: 0.8343\n",
      "Test Loss: 0.915177, Acc: 0.570451, Pre: 0.712443, Rec: 0.734050, F1: 0.720442\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.559540\n",
      "Train F1: 0.747584\n",
      "epoch: 11, loss: 0.8904\n",
      "Test Loss: 0.909235, Acc: 0.568931, Pre: 0.705639, Rec: 0.716272, F1: 0.710854\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.564788\n",
      "Train F1: 0.758553\n",
      "epoch: 12, loss: 0.8007\n",
      "Test Loss: 0.884099, Acc: 0.587684, Pre: 0.726550, Rec: 0.733633, F1: 0.729783\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.568223\n",
      "Train F1: 0.764620\n",
      "epoch: 13, loss: 0.7492\n",
      "Test Loss: 0.887364, Acc: 0.598581, Pre: 0.735798, Rec: 0.736256, F1: 0.732923\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.572412\n",
      "Train F1: 0.770143\n",
      "epoch: 14, loss: 0.716\n",
      "Test Loss: 0.866286, Acc: 0.607704, Pre: 0.741969, Rec: 0.752421, F1: 0.744132\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.576765\n",
      "Train F1: 0.780063\n",
      "epoch: 15, loss: 0.7098\n",
      "Test Loss: 0.875210, Acc: 0.589458, Pre: 0.728767, Rec: 0.724716, F1: 0.726718\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.578872\n",
      "Train F1: 0.782496\n",
      "epoch: 16, loss: 0.7676\n",
      "Test Loss: 0.868855, Acc: 0.600608, Pre: 0.742635, Rec: 0.727199, F1: 0.733753\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.581486\n",
      "Train F1: 0.787331\n",
      "epoch: 17, loss: 0.7103\n",
      "Test Loss: 0.873366, Acc: 0.592752, Pre: 0.729465, Rec: 0.728759, F1: 0.729106\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.584754\n",
      "Train F1: 0.791458\n",
      "epoch: 18, loss: 0.7323\n",
      "Test Loss: 0.860870, Acc: 0.599848, Pre: 0.727710, Rec: 0.767142, F1: 0.746473\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.587173\n",
      "Train F1: 0.796398\n",
      "epoch: 19, loss: 0.6687\n",
      "Test Loss: 0.837255, Acc: 0.623670, Pre: 0.748567, Rec: 0.769319, F1: 0.758668\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.590170\n",
      "Train F1: 0.802757\n",
      "epoch: 20, loss: 0.6934\n",
      "Test Loss: 0.830768, Acc: 0.629245, Pre: 0.768638, Rec: 0.741246, F1: 0.754480\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.593701\n",
      "Train F1: 0.809055\n",
      "epoch: 21, loss: 0.7327\n",
      "Test Loss: 0.813727, Acc: 0.632539, Pre: 0.772798, Rec: 0.751515, F1: 0.761505\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.594669\n",
      "Train F1: 0.812456\n",
      "epoch: 22, loss: 0.6033\n",
      "Test Loss: 0.793038, Acc: 0.641409, Pre: 0.764626, Rec: 0.774350, F1: 0.769145\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.597528\n",
      "Train F1: 0.814055\n",
      "epoch: 23, loss: 0.5078\n",
      "Test Loss: 0.811022, Acc: 0.641916, Pre: 0.753065, Rec: 0.795810, F1: 0.771722\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.600364\n",
      "Train F1: 0.819562\n",
      "epoch: 24, loss: 0.664\n",
      "Test Loss: 0.831516, Acc: 0.636847, Pre: 0.781515, Rec: 0.739933, F1: 0.759379\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.603092\n",
      "Train F1: 0.824989\n",
      "epoch: 25, loss: 0.7567\n",
      "Test Loss: 0.810455, Acc: 0.648758, Pre: 0.780680, Rec: 0.760960, F1: 0.770129\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.604459\n",
      "Train F1: 0.827004\n",
      "epoch: 26, loss: 0.6672\n",
      "Test Loss: 0.816418, Acc: 0.641662, Pre: 0.770616, Rec: 0.760606, F1: 0.765518\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.606855\n",
      "Train F1: 0.832202\n",
      "epoch: 27, loss: 0.6804\n",
      "Test Loss: 0.776920, Acc: 0.661936, Pre: 0.770818, Rec: 0.814484, F1: 0.791199\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.611228\n",
      "Train F1: 0.840051\n",
      "epoch: 28, loss: 0.6607\n",
      "Test Loss: 0.781378, Acc: 0.660669, Pre: 0.790206, Rec: 0.763361, F1: 0.775599\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.613638\n",
      "Train F1: 0.842740\n",
      "epoch: 29, loss: 0.535\n",
      "Test Loss: 0.815975, Acc: 0.645464, Pre: 0.762687, Rec: 0.784149, F1: 0.772808\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.613810\n",
      "Train F1: 0.845157\n",
      "epoch: 30, loss: 0.6076\n",
      "Test Loss: 0.792800, Acc: 0.662190, Pre: 0.778145, Rec: 0.786181, F1: 0.781317\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.614061\n",
      "Train F1: 0.846464\n",
      "epoch: 31, loss: 0.5959\n",
      "Test Loss: 0.778653, Acc: 0.667258, Pre: 0.784048, Rec: 0.786415, F1: 0.784601\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.617099\n",
      "Train F1: 0.851071\n",
      "epoch: 32, loss: 0.748\n",
      "Test Loss: 0.767815, Acc: 0.671059, Pre: 0.787575, Rec: 0.790382, F1: 0.788918\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.619577\n",
      "Train F1: 0.854450\n",
      "epoch: 33, loss: 0.3776\n",
      "Test Loss: 0.828934, Acc: 0.652813, Pre: 0.764271, Rec: 0.795464, F1: 0.777689\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.619488\n",
      "Train F1: 0.855257\n",
      "epoch: 34, loss: 0.5575\n",
      "Test Loss: 0.778399, Acc: 0.676128, Pre: 0.787373, Rec: 0.795447, F1: 0.789329\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.622354\n",
      "Train F1: 0.861191\n",
      "epoch: 35, loss: 0.6336\n",
      "Test Loss: 0.773494, Acc: 0.676635, Pre: 0.784434, Rec: 0.813517, F1: 0.798247\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.624438\n",
      "Train F1: 0.863518\n",
      "epoch: 36, loss: 0.4795\n",
      "Test Loss: 0.749223, Acc: 0.683984, Pre: 0.791662, Rec: 0.818536, F1: 0.804662\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.626273\n",
      "Train F1: 0.867558\n",
      "epoch: 37, loss: 0.5375\n",
      "Test Loss: 0.759254, Acc: 0.688545, Pre: 0.800079, Rec: 0.805925, F1: 0.802586\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.626366\n",
      "Train F1: 0.868697\n",
      "epoch: 38, loss: 0.4935\n",
      "Test Loss: 0.818973, Acc: 0.655094, Pre: 0.786643, Rec: 0.766461, F1: 0.774594\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.628676\n",
      "Train F1: 0.872690\n",
      "epoch: 39, loss: 0.4404\n",
      "Test Loss: 0.787998, Acc: 0.687278, Pre: 0.803102, Rec: 0.782307, F1: 0.790658\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.630966\n",
      "Train F1: 0.878847\n",
      "epoch: 40, loss: 0.402\n",
      "Test Loss: 0.761957, Acc: 0.684491, Pre: 0.793257, Rec: 0.816833, F1: 0.804524\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.631348\n",
      "Train F1: 0.880944\n",
      "epoch: 41, loss: 0.5432\n",
      "Test Loss: 0.739835, Acc: 0.708566, Pre: 0.811821, Rec: 0.814483, F1: 0.811458\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.631671\n",
      "Train F1: 0.882468\n",
      "epoch: 42, loss: 0.2867\n",
      "Test Loss: 0.747640, Acc: 0.691586, Pre: 0.801198, Rec: 0.801830, F1: 0.801382\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.631762\n",
      "Train F1: 0.882681\n",
      "epoch: 43, loss: 0.4183\n",
      "Test Loss: 0.769885, Acc: 0.679422, Pre: 0.800981, Rec: 0.788556, F1: 0.793826\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.633386\n",
      "Train F1: 0.886055\n",
      "epoch: 44, loss: 0.386\n",
      "Test Loss: 0.738380, Acc: 0.700710, Pre: 0.819713, Rec: 0.786014, F1: 0.801362\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.633966\n",
      "Train F1: 0.887989\n",
      "epoch: 45, loss: 0.4169\n",
      "Test Loss: 0.736662, Acc: 0.705018, Pre: 0.805380, Rec: 0.822489, F1: 0.812980\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.634743\n",
      "Train F1: 0.889050\n",
      "epoch: 46, loss: 0.3489\n",
      "Test Loss: 0.725028, Acc: 0.714394, Pre: 0.816861, Rec: 0.815412, F1: 0.815348\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.637387\n",
      "Train F1: 0.895974\n",
      "epoch: 47, loss: 0.4461\n",
      "Test Loss: 0.749387, Acc: 0.698429, Pre: 0.805655, Rec: 0.804468, F1: 0.804951\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.638122\n",
      "Train F1: 0.897145\n",
      "epoch: 48, loss: 0.3288\n",
      "Test Loss: 0.755544, Acc: 0.701723, Pre: 0.809492, Rec: 0.802662, F1: 0.805944\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.637677\n",
      "Train F1: 0.896079\n",
      "epoch: 49, loss: 0.3621\n",
      "Test Loss: 0.750418, Acc: 0.701977, Pre: 0.807921, Rec: 0.817228, F1: 0.812524\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.638427\n",
      "Train F1: 0.900112\n",
      "epoch: 50, loss: 0.4326\n",
      "Test Loss: 0.765702, Acc: 0.710593, Pre: 0.817274, Rec: 0.809196, F1: 0.811335\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.640798\n",
      "Train F1: 0.903018\n",
      "epoch: 51, loss: 0.3998\n",
      "Test Loss: 0.745421, Acc: 0.716929, Pre: 0.812766, Rec: 0.826669, F1: 0.819427\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.640619\n",
      "Train F1: 0.904717\n",
      "epoch: 52, loss: 0.5356\n",
      "Test Loss: 0.726942, Acc: 0.721237, Pre: 0.829185, Rec: 0.811646, F1: 0.819342\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.639276\n",
      "Train F1: 0.903311\n",
      "epoch: 53, loss: 0.4725\n",
      "Test Loss: 0.741261, Acc: 0.720223, Pre: 0.825743, Rec: 0.813418, F1: 0.818256\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.641093\n",
      "Train F1: 0.906242\n",
      "epoch: 54, loss: 0.5818\n",
      "Test Loss: 0.768473, Acc: 0.715155, Pre: 0.824702, Rec: 0.806529, F1: 0.814953\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.641606\n",
      "Train F1: 0.907050\n",
      "epoch: 55, loss: 0.2413\n",
      "Test Loss: 0.745023, Acc: 0.719463, Pre: 0.825355, Rec: 0.808821, F1: 0.816236\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.641936\n",
      "Train F1: 0.909695\n",
      "epoch: 56, loss: 0.4496\n",
      "Test Loss: 0.854449, Acc: 0.688292, Pre: 0.792362, Rec: 0.793876, F1: 0.793116\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.643184\n",
      "Train F1: 0.910336\n",
      "epoch: 57, loss: 0.6319\n",
      "Test Loss: 0.780701, Acc: 0.722757, Pre: 0.819569, Rec: 0.821343, F1: 0.818382\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.642714\n",
      "Train F1: 0.909574\n",
      "epoch: 58, loss: 0.3751\n",
      "Test Loss: 0.723248, Acc: 0.731880, Pre: 0.829425, Rec: 0.833179, F1: 0.830558\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.642507\n",
      "Train F1: 0.911000\n",
      "epoch: 59, loss: 0.3088\n",
      "Test Loss: 0.773535, Acc: 0.727826, Pre: 0.835116, Rec: 0.807056, F1: 0.816614\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.643121\n",
      "Train F1: 0.913798\n",
      "epoch: 60, loss: 0.5373\n",
      "Test Loss: 0.779166, Acc: 0.716675, Pre: 0.837170, Rec: 0.791000, F1: 0.812586\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.645172\n",
      "Train F1: 0.915578\n",
      "epoch: 61, loss: 0.3514\n",
      "Test Loss: 0.795084, Acc: 0.724278, Pre: 0.838553, Rec: 0.797317, F1: 0.814020\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.644751\n",
      "Train F1: 0.916444\n",
      "epoch: 62, loss: 0.2808\n",
      "Test Loss: 0.795016, Acc: 0.717942, Pre: 0.817569, Rec: 0.821100, F1: 0.818192\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.645729\n",
      "Train F1: 0.917892\n",
      "epoch: 63, loss: 0.3709\n",
      "Test Loss: 0.769080, Acc: 0.732641, Pre: 0.836691, Rec: 0.816401, F1: 0.825767\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.645348\n",
      "Train F1: 0.919300\n",
      "epoch: 64, loss: 0.1691\n",
      "Test Loss: 0.775270, Acc: 0.728079, Pre: 0.837793, Rec: 0.804543, F1: 0.819649\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.645745\n",
      "Train F1: 0.920771\n",
      "epoch: 65, loss: 0.2932\n",
      "Test Loss: 0.770645, Acc: 0.736189, Pre: 0.830813, Rec: 0.829877, F1: 0.829448\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.646922\n",
      "Train F1: 0.923937\n",
      "epoch: 66, loss: 0.1677\n",
      "Test Loss: 0.760312, Acc: 0.731374, Pre: 0.827229, Rec: 0.835399, F1: 0.830925\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.647427\n",
      "Train F1: 0.922901\n",
      "epoch: 67, loss: 0.3886\n",
      "Test Loss: 0.791524, Acc: 0.722504, Pre: 0.815360, Rec: 0.840470, F1: 0.827461\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.646535\n",
      "Train F1: 0.923070\n",
      "epoch: 68, loss: 0.5343\n",
      "Test Loss: 0.776897, Acc: 0.723771, Pre: 0.826764, Rec: 0.819064, F1: 0.822858\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.646545\n",
      "Train F1: 0.922312\n",
      "epoch: 69, loss: 0.2604\n",
      "Test Loss: 0.773200, Acc: 0.736949, Pre: 0.829659, Rec: 0.833967, F1: 0.830907\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.646777\n",
      "Train F1: 0.924437\n",
      "epoch: 70, loss: 0.2085\n",
      "Test Loss: 0.802164, Acc: 0.732641, Pre: 0.822854, Rec: 0.841325, F1: 0.831036\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.648640\n",
      "Train F1: 0.927004\n",
      "epoch: 71, loss: 0.36\n",
      "Test Loss: 0.761889, Acc: 0.746325, Pre: 0.841669, Rec: 0.835637, F1: 0.838123\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.649033\n",
      "Train F1: 0.927369\n",
      "epoch: 72, loss: 0.272\n",
      "Test Loss: 0.798046, Acc: 0.740243, Pre: 0.834163, Rec: 0.832501, F1: 0.831445\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.649649\n",
      "Train F1: 0.929810\n",
      "epoch: 73, loss: 0.3995\n",
      "Test Loss: 0.802080, Acc: 0.744551, Pre: 0.836638, Rec: 0.832019, F1: 0.832083\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.647927\n",
      "Train F1: 0.927199\n",
      "epoch: 74, loss: 0.306\n",
      "Test Loss: 0.785302, Acc: 0.727572, Pre: 0.823625, Rec: 0.830708, F1: 0.827140\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.648335\n",
      "Train F1: 0.928377\n",
      "epoch: 75, loss: 0.2868\n",
      "Test Loss: 0.825082, Acc: 0.723771, Pre: 0.820241, Rec: 0.823585, F1: 0.821906\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.650628\n",
      "Train F1: 0.932740\n",
      "epoch: 76, loss: 0.3639\n",
      "Test Loss: 0.799138, Acc: 0.733908, Pre: 0.826467, Rec: 0.838462, F1: 0.831974\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.650489\n",
      "Train F1: 0.932616\n",
      "epoch: 77, loss: 0.1607\n",
      "Test Loss: 0.845089, Acc: 0.734415, Pre: 0.836236, Rec: 0.814453, F1: 0.823382\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.650397\n",
      "Train F1: 0.933288\n",
      "epoch: 78, loss: 0.2618\n",
      "Test Loss: 0.807775, Acc: 0.744045, Pre: 0.843341, Rec: 0.824023, F1: 0.832642\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.647812\n",
      "Train F1: 0.928342\n",
      "epoch: 79, loss: 0.2526\n",
      "Test Loss: 0.843209, Acc: 0.734415, Pre: 0.833858, Rec: 0.819749, F1: 0.825837\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.649223\n",
      "Train F1: 0.932359\n",
      "epoch: 80, loss: 0.3936\n",
      "Test Loss: 0.814845, Acc: 0.733654, Pre: 0.828499, Rec: 0.833439, F1: 0.830223\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.649599\n",
      "Train F1: 0.932911\n",
      "epoch: 81, loss: 0.2145\n",
      "Test Loss: 0.809584, Acc: 0.730106, Pre: 0.827804, Rec: 0.820325, F1: 0.823992\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.651504\n",
      "Train F1: 0.936762\n",
      "epoch: 82, loss: 0.3134\n",
      "Test Loss: 0.832070, Acc: 0.745819, Pre: 0.838056, Rec: 0.832087, F1: 0.832190\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.650248\n",
      "Train F1: 0.934653\n",
      "epoch: 83, loss: 0.3569\n",
      "Test Loss: 0.814283, Acc: 0.741510, Pre: 0.838313, Rec: 0.829435, F1: 0.833771\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.652182\n",
      "Train F1: 0.939079\n",
      "epoch: 84, loss: 0.2687\n",
      "Test Loss: 0.869446, Acc: 0.742271, Pre: 0.847703, Rec: 0.813690, F1: 0.824899\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.649366\n",
      "Train F1: 0.933027\n",
      "epoch: 85, loss: 0.3184\n",
      "Test Loss: 0.850874, Acc: 0.740750, Pre: 0.833287, Rec: 0.830988, F1: 0.830202\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.651355\n",
      "Train F1: 0.937507\n",
      "epoch: 86, loss: 0.4039\n",
      "Test Loss: 0.861880, Acc: 0.737962, Pre: 0.830300, Rec: 0.825569, F1: 0.827161\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.650173\n",
      "Train F1: 0.936197\n",
      "epoch: 87, loss: 0.2705\n",
      "Test Loss: 0.844271, Acc: 0.743791, Pre: 0.835555, Rec: 0.837328, F1: 0.834740\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.652594\n",
      "Train F1: 0.939826\n",
      "epoch: 88, loss: 0.1991\n",
      "Test Loss: 0.849339, Acc: 0.733654, Pre: 0.837964, Rec: 0.810369, F1: 0.823778\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.651109\n",
      "Train F1: 0.936860\n",
      "epoch: 89, loss: 0.2659\n",
      "Test Loss: 0.835883, Acc: 0.735175, Pre: 0.827964, Rec: 0.840678, F1: 0.834146\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.651480\n",
      "Train F1: 0.937249\n",
      "epoch: 90, loss: 0.2163\n",
      "Test Loss: 0.855511, Acc: 0.733401, Pre: 0.822975, Rec: 0.845331, F1: 0.833764\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.652616\n",
      "Train F1: 0.941223\n",
      "epoch: 91, loss: 0.256\n",
      "Test Loss: 0.853611, Acc: 0.751140, Pre: 0.842490, Rec: 0.831743, F1: 0.835300\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.653921\n",
      "Train F1: 0.943870\n",
      "epoch: 92, loss: 0.1649\n",
      "Test Loss: 0.864964, Acc: 0.740243, Pre: 0.820921, Rec: 0.866394, F1: 0.842006\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.652298\n",
      "Train F1: 0.940543\n",
      "epoch: 93, loss: 0.3121\n",
      "Test Loss: 0.863996, Acc: 0.752661, Pre: 0.844397, Rec: 0.837592, F1: 0.838568\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.650006\n",
      "Train F1: 0.936394\n",
      "epoch: 94, loss: 0.3126\n",
      "Test Loss: 0.865225, Acc: 0.744805, Pre: 0.843781, Rec: 0.824931, F1: 0.833961\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.653160\n",
      "Train F1: 0.942741\n",
      "epoch: 95, loss: 0.373\n",
      "Test Loss: 0.850357, Acc: 0.753421, Pre: 0.848399, Rec: 0.834026, F1: 0.839524\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.652430\n",
      "Train F1: 0.942025\n",
      "epoch: 96, loss: 0.3914\n",
      "Test Loss: 0.929408, Acc: 0.732641, Pre: 0.848726, Rec: 0.794762, F1: 0.819915\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.652539\n",
      "Train F1: 0.942613\n",
      "epoch: 97, loss: 0.2554\n",
      "Test Loss: 0.860018, Acc: 0.749366, Pre: 0.846279, Rec: 0.833366, F1: 0.839271\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.654224\n",
      "Train F1: 0.946413\n",
      "epoch: 98, loss: 0.4903\n",
      "Test Loss: 0.871091, Acc: 0.759503, Pre: 0.854702, Rec: 0.832573, F1: 0.840784\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.651350\n",
      "Train F1: 0.940313\n",
      "epoch: 99, loss: 0.3873\n",
      "Test Loss: 0.965477, Acc: 0.745565, Pre: 0.840800, Rec: 0.827630, F1: 0.828528\n",
      "The  3  th fold cross validation:\n",
      "Train F1: 0.000000\n",
      "Train F1: 0.036838\n",
      "Train F1: 0.057283\n",
      "epoch: 0, loss: 1.371\n",
      "Test Loss: 1.305322, Acc: 0.335783, Pre: 0.778825, Rec: 0.665410, F1: 0.501015\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.335485\n",
      "Train F1: 0.436774\n",
      "epoch: 1, loss: 1.118\n",
      "Test Loss: 1.119193, Acc: 0.333502, Pre: 0.822034, Rec: 0.347644, F1: 0.359621\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.447031\n",
      "Train F1: 0.567214\n",
      "epoch: 2, loss: 1.087\n",
      "Test Loss: 1.103522, Acc: 0.297516, Pre: 0.710751, Rec: 0.592279, F1: 0.479586\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.472538\n",
      "Train F1: 0.598508\n",
      "epoch: 3, loss: 1.007\n",
      "Test Loss: 1.105418, Acc: 0.383426, Pre: 0.620309, Rec: 0.635762, F1: 0.603377\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.521561\n",
      "Train F1: 0.613797\n",
      "epoch: 4, loss: 1.048\n",
      "Test Loss: 1.115944, Acc: 0.399392, Pre: 0.618752, Rec: 0.666843, F1: 0.626205\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.527753\n",
      "Train F1: 0.653390\n",
      "epoch: 5, loss: 1.03\n",
      "Test Loss: 1.068278, Acc: 0.423720, Pre: 0.635118, Rec: 0.694079, F1: 0.655971\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.532079\n",
      "Train F1: 0.678048\n",
      "epoch: 6, loss: 0.9644\n",
      "Test Loss: 1.036261, Acc: 0.448809, Pre: 0.648659, Rec: 0.701484, F1: 0.669295\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.537412\n",
      "Train F1: 0.690752\n",
      "epoch: 7, loss: 0.8766\n",
      "Test Loss: 1.006479, Acc: 0.487329, Pre: 0.673980, Rec: 0.741709, F1: 0.699603\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.544157\n",
      "Train F1: 0.707583\n",
      "epoch: 8, loss: 0.8293\n",
      "Test Loss: 1.003015, Acc: 0.498226, Pre: 0.688868, Rec: 0.727959, F1: 0.698254\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.552284\n",
      "Train F1: 0.720721\n",
      "epoch: 9, loss: 0.8809\n",
      "Test Loss: 0.968461, Acc: 0.499240, Pre: 0.675216, Rec: 0.739508, F1: 0.704463\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.558868\n",
      "Train F1: 0.735374\n",
      "epoch: 10, loss: 0.7806\n",
      "Test Loss: 0.971054, Acc: 0.513938, Pre: 0.689925, Rec: 0.753243, F1: 0.715829\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.565710\n",
      "Train F1: 0.747988\n",
      "epoch: 11, loss: 0.9258\n",
      "Test Loss: 0.970668, Acc: 0.527876, Pre: 0.688156, Rec: 0.754129, F1: 0.717019\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.568042\n",
      "Train F1: 0.754831\n",
      "epoch: 12, loss: 0.8777\n",
      "Test Loss: 0.973434, Acc: 0.527623, Pre: 0.698066, Rec: 0.716683, F1: 0.706110\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.572937\n",
      "Train F1: 0.763746\n",
      "epoch: 13, loss: 0.651\n",
      "Test Loss: 0.924329, Acc: 0.566143, Pre: 0.713983, Rec: 0.771862, F1: 0.739354\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.578898\n",
      "Train F1: 0.775386\n",
      "epoch: 14, loss: 0.627\n",
      "Test Loss: 0.930090, Acc: 0.545109, Pre: 0.703735, Rec: 0.755779, F1: 0.727602\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.581772\n",
      "Train F1: 0.781702\n",
      "epoch: 15, loss: 0.6614\n",
      "Test Loss: 0.890339, Acc: 0.587937, Pre: 0.731119, Rec: 0.765803, F1: 0.745640\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.583982\n",
      "Train F1: 0.785909\n",
      "epoch: 16, loss: 0.7558\n",
      "Test Loss: 0.917908, Acc: 0.564369, Pre: 0.713592, Rec: 0.773520, F1: 0.741312\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.588436\n",
      "Train F1: 0.792140\n",
      "epoch: 17, loss: 0.7487\n",
      "Test Loss: 0.926345, Acc: 0.565129, Pre: 0.712194, Rec: 0.750503, F1: 0.727590\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.590454\n",
      "Train F1: 0.796759\n",
      "epoch: 18, loss: 0.7401\n",
      "Test Loss: 0.915907, Acc: 0.580335, Pre: 0.722744, Rec: 0.747862, F1: 0.733638\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.594371\n",
      "Train F1: 0.804556\n",
      "epoch: 19, loss: 0.6639\n",
      "Test Loss: 0.868487, Acc: 0.599595, Pre: 0.732401, Rec: 0.799104, F1: 0.763034\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.597335\n",
      "Train F1: 0.810864\n",
      "epoch: 20, loss: 0.7615\n",
      "Test Loss: 0.835833, Acc: 0.622402, Pre: 0.752536, Rec: 0.770665, F1: 0.761378\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.599976\n",
      "Train F1: 0.814209\n",
      "epoch: 21, loss: 0.7618\n",
      "Test Loss: 0.905270, Acc: 0.588951, Pre: 0.723841, Rec: 0.766570, F1: 0.743329\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.601909\n",
      "Train F1: 0.819370\n",
      "epoch: 22, loss: 0.6021\n",
      "Test Loss: 0.853700, Acc: 0.606690, Pre: 0.736399, Rec: 0.783144, F1: 0.758486\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.603814\n",
      "Train F1: 0.822518\n",
      "epoch: 23, loss: 0.4361\n",
      "Test Loss: 0.830493, Acc: 0.631019, Pre: 0.766514, Rec: 0.775261, F1: 0.768744\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.606819\n",
      "Train F1: 0.825815\n",
      "epoch: 24, loss: 0.5762\n",
      "Test Loss: 0.866300, Acc: 0.601875, Pre: 0.741427, Rec: 0.767389, F1: 0.753946\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.609101\n",
      "Train F1: 0.831600\n",
      "epoch: 25, loss: 0.5112\n",
      "Test Loss: 0.847011, Acc: 0.627471, Pre: 0.753267, Rec: 0.806210, F1: 0.776735\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.610193\n",
      "Train F1: 0.832013\n",
      "epoch: 26, loss: 0.6371\n",
      "Test Loss: 0.823238, Acc: 0.636087, Pre: 0.768390, Rec: 0.771955, F1: 0.769466\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.613260\n",
      "Train F1: 0.837039\n",
      "epoch: 27, loss: 0.7258\n",
      "Test Loss: 0.837991, Acc: 0.624430, Pre: 0.769054, Rec: 0.760168, F1: 0.763329\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.611827\n",
      "Train F1: 0.835282\n",
      "epoch: 28, loss: 0.4654\n",
      "Test Loss: 0.870095, Acc: 0.620882, Pre: 0.746938, Rec: 0.794608, F1: 0.767764\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.614148\n",
      "Train F1: 0.841063\n",
      "epoch: 29, loss: 0.6511\n",
      "Test Loss: 0.822766, Acc: 0.644703, Pre: 0.771559, Rec: 0.777933, F1: 0.774643\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.617691\n",
      "Train F1: 0.846331\n",
      "epoch: 30, loss: 0.6377\n",
      "Test Loss: 0.821914, Acc: 0.635834, Pre: 0.753354, Rec: 0.803672, F1: 0.777090\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.617966\n",
      "Train F1: 0.848277\n",
      "epoch: 31, loss: 0.4343\n",
      "Test Loss: 0.872749, Acc: 0.619615, Pre: 0.749088, Rec: 0.778930, F1: 0.763418\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.618871\n",
      "Train F1: 0.848565\n",
      "epoch: 32, loss: 0.4929\n",
      "Test Loss: 0.800653, Acc: 0.653066, Pre: 0.773315, Rec: 0.801452, F1: 0.786913\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.621358\n",
      "Train F1: 0.853423\n",
      "epoch: 33, loss: 0.3683\n",
      "Test Loss: 0.828302, Acc: 0.644450, Pre: 0.756265, Rec: 0.811694, F1: 0.781915\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.621961\n",
      "Train F1: 0.854672\n",
      "epoch: 34, loss: 0.7023\n",
      "Test Loss: 0.827037, Acc: 0.632286, Pre: 0.774310, Rec: 0.755198, F1: 0.764542\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.623466\n",
      "Train F1: 0.858512\n",
      "epoch: 35, loss: 0.6568\n",
      "Test Loss: 0.828355, Acc: 0.640649, Pre: 0.758782, Rec: 0.787699, F1: 0.772697\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.624394\n",
      "Train F1: 0.860916\n",
      "epoch: 36, loss: 0.4335\n",
      "Test Loss: 0.779941, Acc: 0.673847, Pre: 0.785155, Rec: 0.815174, F1: 0.797102\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.626187\n",
      "Train F1: 0.863337\n",
      "epoch: 37, loss: 0.5018\n",
      "Test Loss: 0.742567, Acc: 0.682210, Pre: 0.794788, Rec: 0.798204, F1: 0.794668\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.627177\n",
      "Train F1: 0.864816\n",
      "epoch: 38, loss: 0.7034\n",
      "Test Loss: 0.776916, Acc: 0.670046, Pre: 0.780259, Rec: 0.800600, F1: 0.790088\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.629035\n",
      "Train F1: 0.869195\n",
      "epoch: 39, loss: 0.4951\n",
      "Test Loss: 0.777268, Acc: 0.663203, Pre: 0.779132, Rec: 0.801895, F1: 0.790018\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.629069\n",
      "Train F1: 0.870006\n",
      "epoch: 40, loss: 0.5503\n",
      "Test Loss: 0.821610, Acc: 0.638115, Pre: 0.771924, Rec: 0.794524, F1: 0.781769\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.630764\n",
      "Train F1: 0.873026\n",
      "epoch: 41, loss: 0.6117\n",
      "Test Loss: 0.783021, Acc: 0.675874, Pre: 0.786035, Rec: 0.791375, F1: 0.788632\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.631676\n",
      "Train F1: 0.876460\n",
      "epoch: 42, loss: 0.4936\n",
      "Test Loss: 0.791394, Acc: 0.659655, Pre: 0.769449, Rec: 0.795019, F1: 0.781607\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.633490\n",
      "Train F1: 0.879151\n",
      "epoch: 43, loss: 0.5321\n",
      "Test Loss: 0.765518, Acc: 0.680943, Pre: 0.801951, Rec: 0.783540, F1: 0.792554\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.634487\n",
      "Train F1: 0.881097\n",
      "epoch: 44, loss: 0.5394\n",
      "Test Loss: 0.796498, Acc: 0.684491, Pre: 0.792156, Rec: 0.806720, F1: 0.794856\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.632828\n",
      "Train F1: 0.878505\n",
      "epoch: 45, loss: 0.6565\n",
      "Test Loss: 0.773911, Acc: 0.675114, Pre: 0.784090, Rec: 0.805430, F1: 0.794338\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.635947\n",
      "Train F1: 0.884314\n",
      "epoch: 46, loss: 0.4237\n",
      "Test Loss: 0.740780, Acc: 0.684491, Pre: 0.797726, Rec: 0.792850, F1: 0.795256\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.636517\n",
      "Train F1: 0.886733\n",
      "epoch: 47, loss: 0.5216\n",
      "Test Loss: 0.778549, Acc: 0.681703, Pre: 0.794414, Rec: 0.797496, F1: 0.794968\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.634655\n",
      "Train F1: 0.883165\n",
      "epoch: 48, loss: 0.3599\n",
      "Test Loss: 0.794367, Acc: 0.657121, Pre: 0.775621, Rec: 0.796138, F1: 0.785623\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.637859\n",
      "Train F1: 0.889100\n",
      "epoch: 49, loss: 0.7165\n",
      "Test Loss: 0.742834, Acc: 0.697162, Pre: 0.800195, Rec: 0.821577, F1: 0.808712\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.639262\n",
      "Train F1: 0.891439\n",
      "epoch: 50, loss: 0.3451\n",
      "Test Loss: 0.724639, Acc: 0.697415, Pre: 0.802305, Rec: 0.817262, F1: 0.809591\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.639012\n",
      "Train F1: 0.893426\n",
      "epoch: 51, loss: 0.4066\n",
      "Test Loss: 0.756237, Acc: 0.692093, Pre: 0.789405, Rec: 0.824509, F1: 0.805240\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.639289\n",
      "Train F1: 0.892499\n",
      "epoch: 52, loss: 0.3977\n",
      "Test Loss: 0.751956, Acc: 0.697415, Pre: 0.802925, Rec: 0.822834, F1: 0.810413\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.638800\n",
      "Train F1: 0.891889\n",
      "epoch: 53, loss: 0.3196\n",
      "Test Loss: 0.794615, Acc: 0.688292, Pre: 0.799569, Rec: 0.823924, F1: 0.807946\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.637258\n",
      "Train F1: 0.891122\n",
      "epoch: 54, loss: 0.4014\n",
      "Test Loss: 0.716830, Acc: 0.709579, Pre: 0.803873, Rec: 0.828190, F1: 0.815511\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.640721\n",
      "Train F1: 0.896012\n",
      "epoch: 55, loss: 0.4056\n",
      "Test Loss: 0.715516, Acc: 0.708312, Pre: 0.812168, Rec: 0.818591, F1: 0.815209\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.641378\n",
      "Train F1: 0.898378\n",
      "epoch: 56, loss: 0.4392\n",
      "Test Loss: 0.727667, Acc: 0.697922, Pre: 0.807243, Rec: 0.808975, F1: 0.808083\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.642075\n",
      "Train F1: 0.899785\n",
      "epoch: 57, loss: 0.4663\n",
      "Test Loss: 0.750130, Acc: 0.703244, Pre: 0.815208, Rec: 0.802315, F1: 0.807482\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.641461\n",
      "Train F1: 0.899697\n",
      "epoch: 58, loss: 0.4984\n",
      "Test Loss: 0.758325, Acc: 0.687532, Pre: 0.801697, Rec: 0.802397, F1: 0.801998\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.643621\n",
      "Train F1: 0.904889\n",
      "epoch: 59, loss: 0.3944\n",
      "Test Loss: 0.736366, Acc: 0.703497, Pre: 0.822612, Rec: 0.797488, F1: 0.808823\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.643348\n",
      "Train F1: 0.903611\n",
      "epoch: 60, loss: 0.4642\n",
      "Test Loss: 0.748573, Acc: 0.700456, Pre: 0.803156, Rec: 0.829378, F1: 0.815293\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.643021\n",
      "Train F1: 0.903424\n",
      "epoch: 61, loss: 0.5555\n",
      "Test Loss: 0.746410, Acc: 0.715155, Pre: 0.828662, Rec: 0.792661, F1: 0.808494\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.642259\n",
      "Train F1: 0.902039\n",
      "epoch: 62, loss: 0.5216\n",
      "Test Loss: 0.781507, Acc: 0.690066, Pre: 0.800248, Rec: 0.802809, F1: 0.800740\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.644890\n",
      "Train F1: 0.908287\n",
      "epoch: 63, loss: 0.4157\n",
      "Test Loss: 0.728758, Acc: 0.703751, Pre: 0.800819, Rec: 0.847376, F1: 0.822946\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.644826\n",
      "Train F1: 0.907335\n",
      "epoch: 64, loss: 0.4105\n",
      "Test Loss: 0.705626, Acc: 0.727065, Pre: 0.850831, Rec: 0.792984, F1: 0.817581\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.643654\n",
      "Train F1: 0.906323\n",
      "epoch: 65, loss: 0.4696\n",
      "Test Loss: 0.836203, Acc: 0.684997, Pre: 0.827696, Rec: 0.757314, F1: 0.788124\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.641580\n",
      "Train F1: 0.901722\n",
      "epoch: 66, loss: 0.4509\n",
      "Test Loss: 0.720683, Acc: 0.714901, Pre: 0.816672, Rec: 0.831714, F1: 0.824062\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.646599\n",
      "Train F1: 0.910675\n",
      "epoch: 67, loss: 0.7105\n",
      "Test Loss: 0.742393, Acc: 0.708312, Pre: 0.810412, Rec: 0.817471, F1: 0.813916\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.644742\n",
      "Train F1: 0.908777\n",
      "epoch: 68, loss: 0.3629\n",
      "Test Loss: 0.751346, Acc: 0.703751, Pre: 0.808994, Rec: 0.806833, F1: 0.807908\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.646687\n",
      "Train F1: 0.912485\n",
      "epoch: 69, loss: 0.3266\n",
      "Test Loss: 0.813541, Acc: 0.681703, Pre: 0.782176, Rec: 0.840895, F1: 0.809727\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.647639\n",
      "Train F1: 0.915413\n",
      "epoch: 70, loss: 0.3136\n",
      "Test Loss: 0.744552, Acc: 0.714394, Pre: 0.832413, Rec: 0.794893, F1: 0.812372\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.647590\n",
      "Train F1: 0.915378\n",
      "epoch: 71, loss: 0.3909\n",
      "Test Loss: 0.732150, Acc: 0.731374, Pre: 0.823780, Rec: 0.830764, F1: 0.826419\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.647171\n",
      "Train F1: 0.914511\n",
      "epoch: 72, loss: 0.4268\n",
      "Test Loss: 0.704473, Acc: 0.725798, Pre: 0.837641, Rec: 0.820701, F1: 0.828940\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.647893\n",
      "Train F1: 0.917191\n",
      "epoch: 73, loss: 0.1844\n",
      "Test Loss: 0.760453, Acc: 0.709833, Pre: 0.811055, Rec: 0.814948, F1: 0.812264\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.646011\n",
      "Train F1: 0.915113\n",
      "epoch: 74, loss: 0.3606\n",
      "Test Loss: 0.727606, Acc: 0.735682, Pre: 0.847012, Rec: 0.805315, F1: 0.822943\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.646776\n",
      "Train F1: 0.914937\n",
      "epoch: 75, loss: 0.3372\n",
      "Test Loss: 0.758343, Acc: 0.713127, Pre: 0.810766, Rec: 0.843668, F1: 0.824935\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.647602\n",
      "Train F1: 0.917386\n",
      "epoch: 76, loss: 0.8335\n",
      "Test Loss: 0.778949, Acc: 0.711100, Pre: 0.814125, Rec: 0.821645, F1: 0.815805\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.644736\n",
      "Train F1: 0.913769\n",
      "epoch: 77, loss: 0.1856\n",
      "Test Loss: 0.749930, Acc: 0.720476, Pre: 0.829774, Rec: 0.806363, F1: 0.816313\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.648264\n",
      "Train F1: 0.917895\n",
      "epoch: 78, loss: 0.331\n",
      "Test Loss: 0.735235, Acc: 0.715155, Pre: 0.817400, Rec: 0.826822, F1: 0.821788\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.648400\n",
      "Train F1: 0.919187\n",
      "epoch: 79, loss: 0.3379\n",
      "Test Loss: 0.779343, Acc: 0.722757, Pre: 0.822711, Rec: 0.828547, F1: 0.822493\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.648240\n",
      "Train F1: 0.918151\n",
      "epoch: 80, loss: 0.3638\n",
      "Test Loss: 0.763763, Acc: 0.716422, Pre: 0.808957, Rec: 0.837917, F1: 0.822689\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.648950\n",
      "Train F1: 0.921047\n",
      "epoch: 81, loss: 0.1905\n",
      "Test Loss: 0.765239, Acc: 0.731627, Pre: 0.842729, Rec: 0.819216, F1: 0.826401\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.648514\n",
      "Train F1: 0.920406\n",
      "epoch: 82, loss: 0.491\n",
      "Test Loss: 0.759500, Acc: 0.724785, Pre: 0.822358, Rec: 0.835006, F1: 0.826661\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.649463\n",
      "Train F1: 0.922592\n",
      "epoch: 83, loss: 0.3131\n",
      "Test Loss: 0.757352, Acc: 0.729853, Pre: 0.840209, Rec: 0.800228, F1: 0.818172\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.650326\n",
      "Train F1: 0.923855\n",
      "epoch: 84, loss: 0.2855\n",
      "Test Loss: 0.777316, Acc: 0.734415, Pre: 0.843552, Rec: 0.807981, F1: 0.821881\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.650057\n",
      "Train F1: 0.925593\n",
      "epoch: 85, loss: 0.2823\n",
      "Test Loss: 0.743717, Acc: 0.718196, Pre: 0.820971, Rec: 0.825037, F1: 0.822708\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.650365\n",
      "Train F1: 0.925013\n",
      "epoch: 86, loss: 0.3049\n",
      "Test Loss: 0.802795, Acc: 0.722250, Pre: 0.821814, Rec: 0.828376, F1: 0.820675\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.648576\n",
      "Train F1: 0.921909\n",
      "epoch: 87, loss: 0.3072\n",
      "Test Loss: 0.745679, Acc: 0.736695, Pre: 0.839362, Rec: 0.823106, F1: 0.829230\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.651685\n",
      "Train F1: 0.928071\n",
      "epoch: 88, loss: 0.3155\n",
      "Test Loss: 0.670940, Acc: 0.768626, Pre: 0.856476, Rec: 0.843779, F1: 0.848565\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.651978\n",
      "Train F1: 0.929024\n",
      "epoch: 89, loss: 0.3362\n",
      "Test Loss: 0.734497, Acc: 0.743538, Pre: 0.847428, Rec: 0.821164, F1: 0.833228\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.650498\n",
      "Train F1: 0.925917\n",
      "epoch: 90, loss: 0.2068\n",
      "Test Loss: 0.739894, Acc: 0.749113, Pre: 0.850445, Rec: 0.818024, F1: 0.831319\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.652036\n",
      "Train F1: 0.929246\n",
      "epoch: 91, loss: 0.3242\n",
      "Test Loss: 0.799502, Acc: 0.736695, Pre: 0.855239, Rec: 0.807222, F1: 0.821626\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.650978\n",
      "Train F1: 0.928463\n",
      "epoch: 92, loss: 0.3502\n",
      "Test Loss: 0.800481, Acc: 0.734161, Pre: 0.849994, Rec: 0.797773, F1: 0.820103\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.650694\n",
      "Train F1: 0.927002\n",
      "epoch: 93, loss: 0.2256\n",
      "Test Loss: 0.789733, Acc: 0.732641, Pre: 0.820319, Rec: 0.853544, F1: 0.834677\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.652185\n",
      "Train F1: 0.929669\n",
      "epoch: 94, loss: 0.4181\n",
      "Test Loss: 0.767049, Acc: 0.720476, Pre: 0.813478, Rec: 0.859808, F1: 0.835515\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.650954\n",
      "Train F1: 0.928114\n",
      "epoch: 95, loss: 0.2247\n",
      "Test Loss: 0.815956, Acc: 0.732387, Pre: 0.835123, Rec: 0.826808, F1: 0.827407\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.651370\n",
      "Train F1: 0.928621\n",
      "epoch: 96, loss: 0.1996\n",
      "Test Loss: 0.722554, Acc: 0.749873, Pre: 0.834253, Rec: 0.850886, F1: 0.841329\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.650609\n",
      "Train F1: 0.928918\n",
      "epoch: 97, loss: 0.2965\n",
      "Test Loss: 0.827347, Acc: 0.728839, Pre: 0.836893, Rec: 0.805387, F1: 0.818650\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.651920\n",
      "Train F1: 0.930299\n",
      "epoch: 98, loss: 0.2862\n",
      "Test Loss: 0.758869, Acc: 0.721997, Pre: 0.826312, Rec: 0.822043, F1: 0.824165\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.652775\n",
      "Train F1: 0.932353\n",
      "epoch: 99, loss: 0.1749\n",
      "Test Loss: 0.766584, Acc: 0.745819, Pre: 0.838462, Rec: 0.843672, F1: 0.838431\n",
      "The  4  th fold cross validation:\n",
      "Train F1: 0.000000\n",
      "Train F1: 0.039333\n",
      "Train F1: 0.061202\n",
      "epoch: 0, loss: 1.356\n",
      "Test Loss: 1.333755, Acc: 0.330461, Pre: 0.776820, Rec: 0.666667, F1: 0.498921\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.494738\n",
      "Train F1: 0.555005\n",
      "epoch: 1, loss: 1.111\n",
      "Test Loss: 1.105812, Acc: 0.377598, Pre: 0.601427, Rec: 0.490784, F1: 0.481275\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.447754\n",
      "Train F1: 0.580517\n",
      "epoch: 2, loss: 1.104\n",
      "Test Loss: 1.081902, Acc: 0.405474, Pre: 0.664603, Rec: 0.671777, F1: 0.565029\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.498564\n",
      "Train F1: 0.635727\n",
      "epoch: 3, loss: 1.067\n",
      "Test Loss: 1.036355, Acc: 0.456665, Pre: 0.684715, Rec: 0.526707, F1: 0.550791\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.516413\n",
      "Train F1: 0.644644\n",
      "epoch: 4, loss: 0.9303\n",
      "Test Loss: 0.964843, Acc: 0.529650, Pre: 0.722502, Rec: 0.645972, F1: 0.658377\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.523492\n",
      "Train F1: 0.670233\n",
      "epoch: 5, loss: 1.08\n",
      "Test Loss: 0.939775, Acc: 0.551191, Pre: 0.742085, Rec: 0.667714, F1: 0.679862\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.530181\n",
      "Train F1: 0.686869\n",
      "epoch: 6, loss: 0.9864\n",
      "Test Loss: 0.906004, Acc: 0.571465, Pre: 0.747241, Rec: 0.689353, F1: 0.698479\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.536664\n",
      "Train F1: 0.696589\n",
      "epoch: 7, loss: 0.8082\n",
      "Test Loss: 0.874084, Acc: 0.590978, Pre: 0.750213, Rec: 0.723948, F1: 0.723156\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.542560\n",
      "Train F1: 0.709634\n",
      "epoch: 8, loss: 0.9056\n",
      "Test Loss: 0.852399, Acc: 0.612266, Pre: 0.763369, Rec: 0.735054, F1: 0.740177\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.551244\n",
      "Train F1: 0.723837\n",
      "epoch: 9, loss: 0.8976\n",
      "Test Loss: 0.851719, Acc: 0.620628, Pre: 0.760466, Rec: 0.743959, F1: 0.744929\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.555311\n",
      "Train F1: 0.733760\n",
      "epoch: 10, loss: 0.8064\n",
      "Test Loss: 0.812872, Acc: 0.632793, Pre: 0.758355, Rec: 0.761183, F1: 0.756726\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.560596\n",
      "Train F1: 0.742044\n",
      "epoch: 11, loss: 0.8297\n",
      "Test Loss: 0.884010, Acc: 0.601875, Pre: 0.770593, Rec: 0.696067, F1: 0.714702\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.564276\n",
      "Train F1: 0.751119\n",
      "epoch: 12, loss: 0.7626\n",
      "Test Loss: 0.825944, Acc: 0.629498, Pre: 0.763244, Rec: 0.772961, F1: 0.757127\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.568329\n",
      "Train F1: 0.759780\n",
      "epoch: 13, loss: 0.7409\n",
      "Test Loss: 0.846769, Acc: 0.634313, Pre: 0.783070, Rec: 0.719199, F1: 0.746559\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.573838\n",
      "Train F1: 0.767384\n",
      "epoch: 14, loss: 0.7196\n",
      "Test Loss: 0.803824, Acc: 0.651799, Pre: 0.781865, Rec: 0.762253, F1: 0.765027\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.578685\n",
      "Train F1: 0.775699\n",
      "epoch: 15, loss: 0.8584\n",
      "Test Loss: 0.798565, Acc: 0.644703, Pre: 0.782347, Rec: 0.738304, F1: 0.755260\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.578337\n",
      "Train F1: 0.777193\n",
      "epoch: 16, loss: 0.9277\n",
      "Test Loss: 0.748610, Acc: 0.669539, Pre: 0.782499, Rec: 0.791109, F1: 0.783774\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.583193\n",
      "Train F1: 0.786731\n",
      "epoch: 17, loss: 0.954\n",
      "Test Loss: 0.765781, Acc: 0.661936, Pre: 0.785928, Rec: 0.778579, F1: 0.777578\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.586758\n",
      "Train F1: 0.791275\n",
      "epoch: 18, loss: 0.7137\n",
      "Test Loss: 0.734612, Acc: 0.681450, Pre: 0.791510, Rec: 0.798142, F1: 0.794114\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.590190\n",
      "Train F1: 0.799259\n",
      "epoch: 19, loss: 0.7362\n",
      "Test Loss: 0.775021, Acc: 0.654587, Pre: 0.784264, Rec: 0.774358, F1: 0.771877\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.590472\n",
      "Train F1: 0.799352\n",
      "epoch: 20, loss: 0.698\n",
      "Test Loss: 0.746343, Acc: 0.669539, Pre: 0.794494, Rec: 0.772887, F1: 0.780916\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.595302\n",
      "Train F1: 0.807495\n",
      "epoch: 21, loss: 0.6211\n",
      "Test Loss: 0.736585, Acc: 0.676381, Pre: 0.795534, Rec: 0.776005, F1: 0.783868\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.597350\n",
      "Train F1: 0.811436\n",
      "epoch: 22, loss: 0.6785\n",
      "Test Loss: 0.786875, Acc: 0.648758, Pre: 0.786143, Rec: 0.767027, F1: 0.762254\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.598257\n",
      "Train F1: 0.815047\n",
      "epoch: 23, loss: 0.7143\n",
      "Test Loss: 0.739461, Acc: 0.672580, Pre: 0.792508, Rec: 0.789916, F1: 0.789426\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.600899\n",
      "Train F1: 0.817239\n",
      "epoch: 24, loss: 0.5573\n",
      "Test Loss: 0.788041, Acc: 0.650786, Pre: 0.802206, Rec: 0.739315, F1: 0.758567\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.604469\n",
      "Train F1: 0.822543\n",
      "epoch: 25, loss: 0.7895\n",
      "Test Loss: 0.799989, Acc: 0.645971, Pre: 0.796688, Rec: 0.752013, F1: 0.756712\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.605159\n",
      "Train F1: 0.825910\n",
      "epoch: 26, loss: 0.5762\n",
      "Test Loss: 0.739662, Acc: 0.674354, Pre: 0.808337, Rec: 0.763872, F1: 0.779702\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.606939\n",
      "Train F1: 0.828728\n",
      "epoch: 27, loss: 0.6867\n",
      "Test Loss: 0.706408, Acc: 0.698429, Pre: 0.822316, Rec: 0.774802, F1: 0.797343\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.610199\n",
      "Train F1: 0.833762\n",
      "epoch: 28, loss: 0.7726\n",
      "Test Loss: 0.721409, Acc: 0.691586, Pre: 0.805888, Rec: 0.790746, F1: 0.796624\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.610355\n",
      "Train F1: 0.835960\n",
      "epoch: 29, loss: 0.4435\n",
      "Test Loss: 0.698275, Acc: 0.694374, Pre: 0.819778, Rec: 0.782161, F1: 0.799546\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.615453\n",
      "Train F1: 0.842387\n",
      "epoch: 30, loss: 0.4982\n",
      "Test Loss: 0.758095, Acc: 0.681703, Pre: 0.818639, Rec: 0.758082, F1: 0.781527\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.615783\n",
      "Train F1: 0.844811\n",
      "epoch: 31, loss: 0.6615\n",
      "Test Loss: 0.695671, Acc: 0.683224, Pre: 0.797056, Rec: 0.814155, F1: 0.801732\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.618960\n",
      "Train F1: 0.849821\n",
      "epoch: 32, loss: 0.7618\n",
      "Test Loss: 0.687983, Acc: 0.700456, Pre: 0.824834, Rec: 0.786375, F1: 0.803083\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.620100\n",
      "Train F1: 0.851460\n",
      "epoch: 33, loss: 0.5202\n",
      "Test Loss: 0.739263, Acc: 0.677395, Pre: 0.807026, Rec: 0.785806, F1: 0.788080\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.621489\n",
      "Train F1: 0.854344\n",
      "epoch: 34, loss: 0.7538\n",
      "Test Loss: 0.675244, Acc: 0.697162, Pre: 0.815572, Rec: 0.784151, F1: 0.797487\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.624349\n",
      "Train F1: 0.860188\n",
      "epoch: 35, loss: 0.7534\n",
      "Test Loss: 0.663533, Acc: 0.709579, Pre: 0.813374, Rec: 0.810570, F1: 0.810456\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.625376\n",
      "Train F1: 0.863017\n",
      "epoch: 36, loss: 0.4704\n",
      "Test Loss: 0.626967, Acc: 0.736442, Pre: 0.824370, Rec: 0.840976, F1: 0.831619\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.625771\n",
      "Train F1: 0.863028\n",
      "epoch: 37, loss: 0.4534\n",
      "Test Loss: 0.683992, Acc: 0.710086, Pre: 0.824958, Rec: 0.791943, F1: 0.807667\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.626093\n",
      "Train F1: 0.863918\n",
      "epoch: 38, loss: 0.3706\n",
      "Test Loss: 0.643365, Acc: 0.719716, Pre: 0.824413, Rec: 0.818145, F1: 0.821118\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.627848\n",
      "Train F1: 0.867979\n",
      "epoch: 39, loss: 0.5461\n",
      "Test Loss: 0.668690, Acc: 0.713127, Pre: 0.836800, Rec: 0.790984, F1: 0.812760\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.629050\n",
      "Train F1: 0.869635\n",
      "epoch: 40, loss: 0.6368\n",
      "Test Loss: 0.762334, Acc: 0.674861, Pre: 0.810151, Rec: 0.773796, F1: 0.780478\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.629367\n",
      "Train F1: 0.871880\n",
      "epoch: 41, loss: 0.634\n",
      "Test Loss: 0.663032, Acc: 0.710340, Pre: 0.821286, Rec: 0.799885, F1: 0.808079\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.630763\n",
      "Train F1: 0.873403\n",
      "epoch: 42, loss: 0.4709\n",
      "Test Loss: 0.645755, Acc: 0.725291, Pre: 0.833401, Rec: 0.813207, F1: 0.822411\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.632696\n",
      "Train F1: 0.877772\n",
      "epoch: 43, loss: 0.3914\n",
      "Test Loss: 0.647563, Acc: 0.728332, Pre: 0.827987, Rec: 0.820952, F1: 0.824422\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.633125\n",
      "Train F1: 0.879946\n",
      "epoch: 44, loss: 0.3713\n",
      "Test Loss: 0.637596, Acc: 0.731374, Pre: 0.826827, Rec: 0.835349, F1: 0.830767\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.633563\n",
      "Train F1: 0.880135\n",
      "epoch: 45, loss: 0.4604\n",
      "Test Loss: 0.643208, Acc: 0.731374, Pre: 0.835784, Rec: 0.813015, F1: 0.823617\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.635067\n",
      "Train F1: 0.884184\n",
      "epoch: 46, loss: 0.2994\n",
      "Test Loss: 0.623342, Acc: 0.738723, Pre: 0.842176, Rec: 0.819210, F1: 0.830394\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.636880\n",
      "Train F1: 0.886166\n",
      "epoch: 47, loss: 0.337\n",
      "Test Loss: 0.635056, Acc: 0.733147, Pre: 0.835475, Rec: 0.824429, F1: 0.829446\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.636193\n",
      "Train F1: 0.886835\n",
      "epoch: 48, loss: 0.6801\n",
      "Test Loss: 0.612523, Acc: 0.742017, Pre: 0.828985, Rec: 0.857256, F1: 0.842685\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.636710\n",
      "Train F1: 0.886814\n",
      "epoch: 49, loss: 0.3714\n",
      "Test Loss: 0.634616, Acc: 0.736189, Pre: 0.826847, Rec: 0.838080, F1: 0.831813\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.636814\n",
      "Train F1: 0.889380\n",
      "epoch: 50, loss: 0.4855\n",
      "Test Loss: 0.638406, Acc: 0.739483, Pre: 0.841310, Rec: 0.821302, F1: 0.830063\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.637966\n",
      "Train F1: 0.890784\n",
      "epoch: 51, loss: 0.4242\n",
      "Test Loss: 0.636498, Acc: 0.724024, Pre: 0.822640, Rec: 0.823846, F1: 0.822093\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.638289\n",
      "Train F1: 0.890943\n",
      "epoch: 52, loss: 0.4108\n",
      "Test Loss: 0.604905, Acc: 0.757223, Pre: 0.841967, Rec: 0.846647, F1: 0.844257\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.639579\n",
      "Train F1: 0.895974\n",
      "epoch: 53, loss: 0.3786\n",
      "Test Loss: 0.659616, Acc: 0.731374, Pre: 0.833673, Rec: 0.811601, F1: 0.821130\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.639543\n",
      "Train F1: 0.894564\n",
      "epoch: 54, loss: 0.3522\n",
      "Test Loss: 0.597098, Acc: 0.750887, Pre: 0.839552, Rec: 0.843913, F1: 0.841193\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.642358\n",
      "Train F1: 0.899892\n",
      "epoch: 55, loss: 0.3507\n",
      "Test Loss: 0.642925, Acc: 0.740497, Pre: 0.842169, Rec: 0.828035, F1: 0.833172\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.642223\n",
      "Train F1: 0.901525\n",
      "epoch: 56, loss: 0.2781\n",
      "Test Loss: 0.619634, Acc: 0.748353, Pre: 0.846805, Rec: 0.828809, F1: 0.837559\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.642624\n",
      "Train F1: 0.902533\n",
      "epoch: 57, loss: 0.3723\n",
      "Test Loss: 0.612982, Acc: 0.764825, Pre: 0.856858, Rec: 0.842863, F1: 0.847978\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.643442\n",
      "Train F1: 0.903201\n",
      "epoch: 58, loss: 0.2942\n",
      "Test Loss: 0.609099, Acc: 0.741004, Pre: 0.829847, Rec: 0.849319, F1: 0.839078\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.643310\n",
      "Train F1: 0.904026\n",
      "epoch: 59, loss: 0.2403\n",
      "Test Loss: 0.632761, Acc: 0.741004, Pre: 0.840870, Rec: 0.828328, F1: 0.833051\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.644457\n",
      "Train F1: 0.905939\n",
      "epoch: 60, loss: 0.3436\n",
      "Test Loss: 0.633390, Acc: 0.750380, Pre: 0.851942, Rec: 0.832337, F1: 0.841794\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.643187\n",
      "Train F1: 0.905209\n",
      "epoch: 61, loss: 0.328\n",
      "Test Loss: 0.637887, Acc: 0.741257, Pre: 0.837651, Rec: 0.824621, F1: 0.830377\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.642758\n",
      "Train F1: 0.904199\n",
      "epoch: 62, loss: 0.2989\n",
      "Test Loss: 0.604701, Acc: 0.764318, Pre: 0.852733, Rec: 0.850302, F1: 0.851157\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.643730\n",
      "Train F1: 0.905222\n",
      "epoch: 63, loss: 0.2143\n",
      "Test Loss: 0.615349, Acc: 0.751394, Pre: 0.856182, Rec: 0.824816, F1: 0.839990\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.645790\n",
      "Train F1: 0.910569\n",
      "epoch: 64, loss: 0.386\n",
      "Test Loss: 0.600153, Acc: 0.761784, Pre: 0.853401, Rec: 0.846354, F1: 0.849853\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.643678\n",
      "Train F1: 0.906669\n",
      "epoch: 65, loss: 0.3838\n",
      "Test Loss: 0.604321, Acc: 0.758996, Pre: 0.858046, Rec: 0.832707, F1: 0.845058\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.646443\n",
      "Train F1: 0.912802\n",
      "epoch: 66, loss: 0.3915\n",
      "Test Loss: 0.601780, Acc: 0.760264, Pre: 0.843522, Rec: 0.851952, F1: 0.847700\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.646570\n",
      "Train F1: 0.913045\n",
      "epoch: 67, loss: 0.4706\n",
      "Test Loss: 0.645256, Acc: 0.737962, Pre: 0.848430, Rec: 0.815393, F1: 0.829837\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.646974\n",
      "Train F1: 0.913935\n",
      "epoch: 68, loss: 0.4092\n",
      "Test Loss: 0.611476, Acc: 0.770654, Pre: 0.861596, Rec: 0.851069, F1: 0.856014\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.648701\n",
      "Train F1: 0.918031\n",
      "epoch: 69, loss: 0.3864\n",
      "Test Loss: 0.606206, Acc: 0.756969, Pre: 0.842298, Rec: 0.849643, F1: 0.845918\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.646740\n",
      "Train F1: 0.914492\n",
      "epoch: 70, loss: 0.2662\n",
      "Test Loss: 0.608399, Acc: 0.765585, Pre: 0.867207, Rec: 0.833903, F1: 0.849050\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.648216\n",
      "Train F1: 0.917656\n",
      "epoch: 71, loss: 0.4739\n",
      "Test Loss: 0.618209, Acc: 0.764572, Pre: 0.863509, Rec: 0.832770, F1: 0.847626\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.646528\n",
      "Train F1: 0.915803\n",
      "epoch: 72, loss: 0.4148\n",
      "Test Loss: 0.614973, Acc: 0.757983, Pre: 0.856774, Rec: 0.830896, F1: 0.843506\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.648943\n",
      "Train F1: 0.918846\n",
      "epoch: 73, loss: 0.2059\n",
      "Test Loss: 0.600479, Acc: 0.758236, Pre: 0.846812, Rec: 0.854372, F1: 0.850201\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.647293\n",
      "Train F1: 0.916135\n",
      "epoch: 74, loss: 0.2188\n",
      "Test Loss: 0.684045, Acc: 0.719716, Pre: 0.825845, Rec: 0.837720, F1: 0.825607\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.647846\n",
      "Train F1: 0.918421\n",
      "epoch: 75, loss: 0.3749\n",
      "Test Loss: 0.676096, Acc: 0.740243, Pre: 0.852395, Rec: 0.810493, F1: 0.830388\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.648055\n",
      "Train F1: 0.918737\n",
      "epoch: 76, loss: 0.307\n",
      "Test Loss: 0.644897, Acc: 0.750127, Pre: 0.856394, Rec: 0.822469, F1: 0.838781\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.648167\n",
      "Train F1: 0.919926\n",
      "epoch: 77, loss: 0.294\n",
      "Test Loss: 0.619346, Acc: 0.757729, Pre: 0.853661, Rec: 0.840896, F1: 0.846637\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.649153\n",
      "Train F1: 0.919838\n",
      "epoch: 78, loss: 0.4457\n",
      "Test Loss: 0.634012, Acc: 0.758743, Pre: 0.852498, Rec: 0.842703, F1: 0.847336\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.647960\n",
      "Train F1: 0.921046\n",
      "epoch: 79, loss: 0.3862\n",
      "Test Loss: 0.616902, Acc: 0.764572, Pre: 0.852300, Rec: 0.847120, F1: 0.849678\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.650523\n",
      "Train F1: 0.923693\n",
      "epoch: 80, loss: 0.329\n",
      "Test Loss: 0.584773, Acc: 0.771921, Pre: 0.854824, Rec: 0.869237, F1: 0.861932\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.651992\n",
      "Train F1: 0.927592\n",
      "epoch: 81, loss: 0.3625\n",
      "Test Loss: 0.613014, Acc: 0.768880, Pre: 0.866187, Rec: 0.834645, F1: 0.849484\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.649276\n",
      "Train F1: 0.923565\n",
      "epoch: 82, loss: 0.5139\n",
      "Test Loss: 0.603988, Acc: 0.771668, Pre: 0.855919, Rec: 0.866706, F1: 0.861251\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.648320\n",
      "Train F1: 0.922193\n",
      "epoch: 83, loss: 0.5107\n",
      "Test Loss: 0.660964, Acc: 0.743031, Pre: 0.852891, Rec: 0.813258, F1: 0.831588\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.650953\n",
      "Train F1: 0.927357\n",
      "epoch: 84, loss: 0.3663\n",
      "Test Loss: 0.642655, Acc: 0.765332, Pre: 0.861264, Rec: 0.829000, F1: 0.844430\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.649720\n",
      "Train F1: 0.923660\n",
      "epoch: 85, loss: 0.2804\n",
      "Test Loss: 0.626100, Acc: 0.750634, Pre: 0.846340, Rec: 0.840649, F1: 0.843032\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.651368\n",
      "Train F1: 0.927867\n",
      "epoch: 86, loss: 0.3192\n",
      "Test Loss: 0.628772, Acc: 0.764825, Pre: 0.864115, Rec: 0.829187, F1: 0.846046\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.653108\n",
      "Train F1: 0.930577\n",
      "epoch: 87, loss: 0.3167\n",
      "Test Loss: 0.602166, Acc: 0.784845, Pre: 0.866852, Rec: 0.866415, F1: 0.865231\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.651967\n",
      "Train F1: 0.928468\n",
      "epoch: 88, loss: 0.2366\n",
      "Test Loss: 0.599862, Acc: 0.769894, Pre: 0.862651, Rec: 0.849046, F1: 0.855515\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.652737\n",
      "Train F1: 0.932666\n",
      "epoch: 89, loss: 0.2159\n",
      "Test Loss: 0.608778, Acc: 0.778003, Pre: 0.861339, Rec: 0.865270, F1: 0.863031\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.651129\n",
      "Train F1: 0.929569\n",
      "epoch: 90, loss: 0.5204\n",
      "Test Loss: 0.655090, Acc: 0.756209, Pre: 0.847415, Rec: 0.850044, F1: 0.848031\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.653042\n",
      "Train F1: 0.931696\n",
      "epoch: 91, loss: 0.2274\n",
      "Test Loss: 0.651289, Acc: 0.762038, Pre: 0.865256, Rec: 0.831540, F1: 0.847586\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.652116\n",
      "Train F1: 0.931961\n",
      "epoch: 92, loss: 0.406\n",
      "Test Loss: 0.630361, Acc: 0.760264, Pre: 0.849728, Rec: 0.867346, F1: 0.857986\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.650537\n",
      "Train F1: 0.927246\n",
      "epoch: 93, loss: 0.2922\n",
      "Test Loss: 0.639622, Acc: 0.764572, Pre: 0.852740, Rec: 0.859033, F1: 0.855762\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.653569\n",
      "Train F1: 0.934097\n",
      "epoch: 94, loss: 0.4068\n",
      "Test Loss: 0.606861, Acc: 0.781804, Pre: 0.869196, Rec: 0.855717, F1: 0.861462\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.652213\n",
      "Train F1: 0.931497\n",
      "epoch: 95, loss: 0.3651\n",
      "Test Loss: 0.609161, Acc: 0.772428, Pre: 0.860528, Rec: 0.855108, F1: 0.857613\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.652428\n",
      "Train F1: 0.932198\n",
      "epoch: 96, loss: 0.3525\n",
      "Test Loss: 0.639136, Acc: 0.771668, Pre: 0.872537, Rec: 0.829042, F1: 0.849339\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.652431\n",
      "Train F1: 0.932885\n",
      "epoch: 97, loss: 0.3183\n",
      "Test Loss: 0.632314, Acc: 0.777750, Pre: 0.862243, Rec: 0.856122, F1: 0.858992\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.651893\n",
      "Train F1: 0.932259\n",
      "epoch: 98, loss: 0.3325\n",
      "Test Loss: 0.640827, Acc: 0.782565, Pre: 0.868223, Rec: 0.859752, F1: 0.863294\n",
      "Train F1: 0.333333\n",
      "Train F1: 0.651788\n",
      "Train F1: 0.932900\n",
      "epoch: 99, loss: 0.2459\n",
      "Test Loss: 0.619321, Acc: 0.768373, Pre: 0.854525, Rec: 0.860038, F1: 0.857203\n"
     ]
    }
   ],
   "source": [
    "# CUDA_LAUNCH_BLOCKING=1\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_validate, KFold\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "learning_rate = 0.002\n",
    "\n",
    "vector_vae = torch.from_numpy(pd.read_csv('vector_vae_balanced.csv').values).float()\n",
    "label = torch.from_numpy(pd.read_csv('label_new_balanced.csv').values).float()\n",
    "\n",
    "totEpoch = 100\n",
    "num_class = 3\n",
    "results, F1_train, F1_test, Loss_train, Loss_test,TPR, FPR, y_pred, y_lable = train_cross_val(100, vector_vae, label, totEpoch, num_class, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5db3cc8-3b78-4fe6-82a4-5855b9bf6fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from numpy import interp\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def prec_rec_curve(y_lable, y_pred, titile=\"\"):\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    \n",
    "    n_classes = 3\n",
    "    y_test = label_binarize(y_lable, classes=[1,2,3])\n",
    "    y_score = label_binarize(y_pred, classes=[1,2,3])\n",
    "    print(y_test)\n",
    "    print(y_score)\n",
    "    print(y_lable)\n",
    "    print(y_pred)\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    try:\n",
    "        micro_auc = roc_auc_score(y_test, y_score, average='micro')\n",
    "    except ValueError:\n",
    "        pass\n",
    "    # micro_auc = roc_auc_score(y_test, y_score, average='micro')\n",
    "    \n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "    mean_tpr /= n_classes\n",
    "\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "    try:\n",
    "        macro_auc = roc_auc_score(y_test, y_score, average='macro')\n",
    "    except ValueError:\n",
    "        pass\n",
    "    # macro_auc = roc_auc_score(y_test, y_score, average='macro')\n",
    "\n",
    "    print(roc_auc)\n",
    "    print('micro auc:', micro_auc)\n",
    "    print('macro auc:', macro_auc)\n",
    "    \n",
    "    # Plot all ROC curves\n",
    "    plt.figure()\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "             label='micro-average ROC curve (area = {0:0.2f})'.format(roc_auc[\"micro\"]),\n",
    "             color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "             label='macro-average ROC curve (area = {0:0.2f})'.format(roc_auc[\"macro\"]),\n",
    "             color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "    colors = ['aqua', 'darkorange', 'cornflowerblue']\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                 label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc[i]))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig('figures/VAE_ROC.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d6753a6-c7cd-4844-9ca3-595181b33954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " ...\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]]\n",
      "[[0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " ...\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]]\n",
      "[2 2 2 ... 3 3 3]\n",
      "[3 2 2 ... 3 3 3]\n",
      "{0: 0.9083504781189191, 1: 0.7693533902110462, 2: 0.8016249263235753, 'micro': 0.8262797769893564, 'macro': 0.8264429315511803}\n",
      "micro auc: 0.8262797769893564\n",
      "macro auc: 0.8264429315511803\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "prec_rec_curve(np.concatenate( y_lable, axis=0 ), np.concatenate( y_pred, axis=0 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23391ec4-88d3-4e38-8084-d5f6f5d8e84a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.87423313, 0.09202454, 0.03374233],\n",
       "       [0.08295626, 0.70588235, 0.21116139],\n",
       "       [0.03191489, 0.24164134, 0.72644377]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "cm = confusion_matrix(np.concatenate( y_lable, axis=0 ), np.concatenate( y_pred, axis=0 ), normalize = 'true')\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7212b6c-d69b-4aa3-9903-677b727328e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAToAAAEYCAYAAADMJjphAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAsm0lEQVR4nO3dd5xU1fnH8c93dwFBmgoIu4sCglJUEGkqYgsKKGIHsVcwtsTEaIzRRGNifpqosQQbEhMVNRaQbu/CAiIKiiCobEGKgEoJy/D8/pi762wfnJmd2dnn7eu+nDv3zLnPjPJwzj33niMzwznn0llGsgNwzrlE80TnnEt7nuicc2nPE51zLu15onPOpT1PdM65tOeJLg1IukzSN5J+kLRHDPX8IKlTPGOrbZLOkjQr2XG41OKJLkVIGi1pbpBsiiRNlzQwis81AP4OHGtmTc1s3U+NIfj88p/6+apI+lLSNkmtyr2/QJJJ6hBFHR2CslnVlTOzJ8zs2BhDdmnGE10KkHQNcDfwZ2BPYC/gAWBEFB/fE9gFWJSo+OJkBXBmyY6kA4DG8TxBTUnQ1V+e6JJMUgvgFuByM3vezDaZWbGZvWRm1wZlGkm6W1JhsN0dvLcvsCSoaoOk1ypr+Uh6Q9LFwevOkt6UtFHSWklPR5QzSZ1L4pL0uKQ1kr6SdKOkjODY+ZLekXSnpPWSVkgaWsNX/TdwbsT+ecDj5X6L4yV9KOk7SSsl/SHi8FsR3/MHSYcEcbwr6S5J3wJ/KIktqO/Q4Du2D/Z7StogqWtN/11cevFEl3yHEG6RvVBNmd8BA4BeQE+gH3CjmX0O9AjKtDSzo6M4363ALGA3IBe4t4py9wItgE7AEYST1AURx/sTTrKtgP8DHpWkas77AdBcUjdJmcBI4D/lymwKztMSOB64TNJJwbFBwb9bBl3s9yPiWA60AW6LrMzM3gMeBP4lqTHhZHujmX1WTZwuDXmiS749gLVmtr2aMmcBt5jZajNbA/wROOcnnq8Y2BvINrOtZvZO+QIRiei3Zva9mX0J/K3cOb8ys4fNLAT8C2hHuBtdnZJW3WDgM6Ag8qCZvWFmH5vZDjNbCDxFOMlWp9DM7jWz7Wa2pZLjfyCcsOcAhcD9NdTn0pAnuuRbB7Sq4fpSNvBVxP5XwXs/xW8AAXMkLZJ0YSVlWgENKzlnTsT+qpIXZrY5eNm0hnP/GxgNnE+5biuApP6SXg+6yxuBsUEs1VlZ3UEzKwYmAPsDfzOfxaJe8kSXfO8DW4GTqilTSLgVVmKv4L3KbAr+3STivbYlL8xslZldYmbZwBjggZLrchHW8mPLL/KcBcTAzL4iPCgxDHi+kiJPApOB9mbWAhhHOCkDVJWgqk1cknKAm4HHgL9JavQTQnd1nCe6JDOzjcBNwP2STpLURFIDSUMl/V9Q7CngRkmtg1s0bqLi9a2S+tYQTkhnS8oMWmz7lByXdLqk3GB3PeFEESpXRwh4BrhNUjNJewPXVHXOnXQRcLSZbarkWDPgWzPbKqkf4dZfiTXADsLXDKMSXDOcADwanLeI8DVKV894oksBZvZ3wonkRsJ/oFcCVwAvBkX+BMwFFgIfA/OD96pyCXAt4W5xD+C9iGN9gdmSfiDcerrazFZUUseVhFuHy4F3CLe2xu/8tyvLzL4ws7lVHP45cIuk7wkn82ciPreZ8GDDu8HI6YAoTncV4euGvw+6rBcAF0g6PKYv4eoc+SUL51y68xadcy7teaJzzqU9T3TOubTnic45l/ZS6iHohmpsjWmW7DBSVpee7ZIdQurLzEx2BCnty6++Zu3atdU9qrfTWmsv28bWqMp+x5qZZjYknuePRkolusY04zBOT3YYKWvKyzcmO4SUl9HM/6KsTp/DBtVcaCcVs5XDo/xzO5UHanrSJSFSKtE55+omEddGYtx5onPOxSzVL/Z7onPOxcxbdM65tCa8ReecS3NCZHqLzjmX7lI7zXmic87FQUaKpzpPdM65mKV2mvNE55yLkQ9GOOfqBR+McM6ltXCLzhOdcy7NpXaa80TnnIuDjGrXLo+QpJUbPNE552IiUr9Fl+qDJc65OiAjyq0mkoZIWiJpmaTrKzneQtJLkj4KFmC/IJr4vEXnnItJvB4Bk5QJ3A8MBvKBPEmTzWxxRLHLgcVmNlxSa2CJpCfMbFt1dXuLzjkXM0X5Tw36AcvMbHmQuCYCI8qVMaBZsDh5U+BbYHtNFXuLzjkXszi1mHIIL95eIh/oX67MfYQXXi8EmgEjzWxHLcXnnKvPFOUGtJI0N2K7tFw15ZUfpz0OWABkA72A+yQ1ryk+b9E552KykzcMrzWzPlUcywfaR+znEm65RboAuN3MDFgmaQXQFZhT3Um9Reeci9lOtOiqkwd0kdRRUkNgFOFuaqSvgWMAJO0J7Acsr6lib9E552IiICsOo65mtl3SFcBMIBMYb2aLJI0Njo8DbgUmSPo4OPV1Zra2pro90TnnYhavG4bNbBowrdx74yJeFwLH7my9nuicczHzh/qdc2mtLjwC5onOORcjeYvOOZfeRHjkIJV5onPOxSy123Oe6JxzceBdV+dcWvPFcZxz9UJqt+dSPxEnxMHHdeehz27ikaV/4PTrBlc43qT5Ltw8eSz3Lfgt//zkRgafPwCAnH3bcO+Hvy3d/rvxTkZcfVRth18rZrz2Ot0OGci+/Q7lr/+4t8JxM+PqG25k336H0uuIY5i/cGHpsX889AgHDjqKAw4/knsefLg2w65VM2a9zH4HHkTnHj25/Y6/VThuZlx1zbV07tGTA/sOYP6HCwDYunUr/QYeSc9+h9Cjd19uvvW2Wo48/uI18WaiJKxFJ2k8cAKw2sz2T9R5dlZGhvj5/Wfwu8H3sjZ/A3fn/YYPJn/Myk9XlZY54fIj+HpxEX88cRzNWzXl4SU38foTeRR8vporD/pLaT2PF/yZ91/4KFlfJWFCoRBXXncDM5+dSG52O/ofO4zhxx1H9/32LS0z/dXXWLp8BUtmv8vsefO5/De/5f0ZU/nk08945D9P8MGMqTRs2JBhI0czbPAxdOnUKYnfKP5CoRCX/+JXvDx1Erk5OfQdeAQnnnA83bt1LS0zfeYsln7xBUs/WcDsOXlcdtUvmf326zRq1IjXZkyhadOmFBcXM/DoYxl67GAG9O+XxG/004VHXVO7TZfIJDsBGJLA+n+Sfft1oHDZGlatWMf24hBvTZzHISMOLFvIjMbNdgGgcdNGfP/tZkLby0551fOY/Vj1xRpWf/1tbYVea+bM/5B9OnagU4e9adiwISNPHsHkGTPLlJk8fSbnnHEakhjQ52A2bNxI0Tff8OnSpfQ/uDdNmjQhKyuLQYcewotTpyfpmyTOnLy5dN6nE506dqRhw4aMOv1UJk2ZUqbMpClTOXf0meHfqH8/NmzcQFHRKiTRtGlTAIqLiyneXoyiXVwmRaV6iy5h5zaztwjP/plS9shpydqV60v31+ZvYI+clmXKvHTfm7Tv1pb/FP6ZBz7+HQ9e/SzhWWF+dMSoPrzx1LzaCLnWFaxaRfuc7NL9nHbtKCgqqlgm+8cyudnZFBStYv+uXXn7/dms+/ZbNm/ezPRXXmNlYfmZduq+gsIi2ufmlO7n5uRQUFDuNyosrFgm+C1CoRC9+h9Km706Mfjoo+jfr2/tBJ4AJYMR9TLRRUvSpSWT8G1jSy2cr+J75ZNY7+O6s3xBPmdn38AVvf7CZfedUdrCA8hqkEn/Ew/gnWfnJzrcpCj/ewAVWhxVlem2bxeuvfLnHHf6KIaNOosDe3QnKyv9xrxi+Y0AMjMzWTD7PfKXfcacufP4ZNHiCmXrkjhN05QwSU90ZvaQmfUxsz4NaZzw863N30Cr9ruV7rfKbcm3hRvLlBl8wQDee34BAEVfrOGbFeto33XP0uN9hvbgi/kr2bD6+4THmwy57dqxsuDHVlhBURHZbdtWLBPRUssvLCS7bfg3uuis0cx9dRZvTH6B3XdrSZeOHWsn8FqUm5PNyvyC0v38ggKys8v9Rjk5Fcu0a1emTMuWLTly0OHMmPVyYgNOsIzgMbCatuTFV898nvcV2V3asGeHPchqkMmgUQfzweSPy5RZ8/V6eh2zHwAt2zQjZ789WbX8xymvjjjzYN58am6txl2b+h7Ui2XLV7Diq6/Ztm0bT78wieHHlZ0ZZ/iQY/n3M//FzPhg7jxaNG9Ouz3DiW71mvBv9XV+Pi9MncaoU06q7a+QcH37HMzSZV+w4ssv2bZtGxOffY4Tjz++TJkTjx/G408+Ff6NZs+hRfMWtGvXljVr1rBhwwYAtmzZwiuvvU7XiIGeuijVW3Tp16eowY7QDv55xTP8aeblZGRmMGv8+3y9uIhhYwYCMO3Bd3jq1ulcM+EcHlh4A0g8dt2LfLduEwCNGjfgoMFduXfMU8n8GgmVlZXFP26/jaEjRxMKhbhg9Ch6dN2PcRMeB2Ds+ecy7GfHMP2VV9m336E0adKYR++5q/Tzp194MevWr6dBVgPuvf3P7NayZZK+SeJkZWVx3113ctzwkwiFdnDheefQo3s3xj38KABjL7mIYUOOY9rMWXTu0ZMmTRrz2IP/BKBo1Tecd8kYQqEQO3bs4IxTT+GEYUOT+XViEp54M7WpsusIcalYego4EmgFfAPcbGaPVveZFmpjh3F6QuJJB1NW35jsEFJeRrNmyQ4hpfU5bBBz582Pa+NqL7Wz63RRVGWvsNvmVbNmRMIkLBGb2ZmJqts5lzri+QiYpCHAPYQnRHnEzG4vd/xa4KxgNwvoBrQ2s2rv8Kh31+icc/EXj9tLJGUC9wNDge7AmZK6R5YxszvMrJeZ9QJ+C7xZU5Iric8552ISp8GIfsAyM1tuZtuAicCIasqfCUR1sdwTnXMuJkJkRrlR/QLWOcDKiP384L2K55SaEH7y6rloYkz1wRLnXB2wEy2m6hawrqzRV9Vo6XDg3Wi6reCJzjkXozgORuQD7SP2c4Gqnh8cRZTdVvCuq3MuDuJ0jS4P6CKpo6SGhJPZ5ArnkloARwCToo3PW3TOuZiEW3Sx35pnZtslXQHMJHx7yXgzWyRpbHC8ZCHrk4FZZrYp2ro90TnnYhavrqGZTQOmlXtvXLn9CYSngYuaJzrnXEx8uUPnXL2Q6hf7PdE552KmFJ9K3ROdcy4mvtyhc65eSO32nCc651wcZGREmepCiY2jKp7onHMxkURmZpSdV090zrm6Sil+kc4TnXMuZhkpvi6tJzrnXEwkULTX6JLEE51zLmZRD0YkiSc651zMUrzn6onOORebnRp1TRJPdM65mHnX1TmX3nwwwjmX7gSkeJ7zROeci12qt+hS+wqicy7llQxGRLNFUdcQSUskLZN0fRVljpS0QNIiSW9GE6O36JxzMYtHi05SJnA/MJjwimB5kiab2eKIMi2BB4AhZva1pDbR1O0tOudcbBS+RhfNVoN+wDIzW25m24CJwIhyZUYDz5vZ1wBmtjqaED3ROedipgxFtQGtJM2N2C6NqCYHWBmxnx+8F2lfYDdJb0iaJ+ncaOLzrqtzLibhUdeou65rzaxPNVWVZ+X2s4CDgWOAxsD7kj4ws8+rO2lKJbouPdsx9dWbkh1Gynq49b3JDiHlXTR3eLJDSG1btsS/zvjdR5cPtI/YzwUKKymzNljTdZOkt4CeQLWJzruuzrmYCJGZGd1Wgzygi6SOkhoCo4DJ5cpMAg6XlCWpCdAf+LSmilOqReecq4MUn0fAzGy7pCuAmYSXih1vZoskjQ2OjzOzTyXNABYCO4BHzOyTmur2ROeci5niNH2JmU0DppV7b1y5/TuAO3amXk90zrmYCH+o3zmX7uRrRjjn6gFfM8I5l9bCo66p3aTzROeci43PR+ecqw/8Gp1zLu2l+CU6T3TOudj4uq7OuXohIzPZEVTPE51zLjZ1YNGIKhOdpHupOEVKKTO7KiEROefqnLp8jW5urUXhnKvT6uyoq5n9K3Jf0q7BHFDOOVdKUsoPRtSYhyUdImkxwZxPknpKeiDhkTnn6gwpui1Zomlw3g0cB6wDMLOPgEEJjMk5V5cIlKWotmSJatTVzFaWm28qlJhwnHN1UV0ejCixUtKhgAXTG19FFFMXO+fqj1QfjIgmvLHA5YSXHSsAegX7zjkXXH9TVFvNdWmIpCWSlkm6vpLjR0raKGlBsEW1mlaNLTozWwucFU1lzrn6KR4tOkmZwP3AYMKrfeVJmmxmi8sVfdvMTtiZuqMZde0k6SVJayStljRJUqedOYlzLs1lRLlVrx+wzMyWm9k2YCIwIl7h1eRJ4BmgHZANPAs8FY+TO+fSgCAjU1FtQCtJcyO2SyNqygFWRuznB++Vd4ikjyRNl9QjmhCjGYyQmf07Yv8/wZJkzjm3s2tGrDWzPlXXVEH5x1DnA3ub2Q+ShgEvAl1qOmmV4UnaXdLuwOuSrpfUQdLekn4DTK2pYudc/SDidsNwPtA+Yj8XKIwsYGbfmdkPwetpQANJrWqquLoW3TzC2bQkvDGR5wNurTFs51z9EJ9HwPKALpI6Er7DYxQwOrKApLbAN2ZmkvoRbqytq6ni6p517RhTyM65+iFOyx2a2fbgsthMIBMYb2aLJI0Njo8DTgMuk7Qd2AKMMrMqZ1kqEdWTEZL2B7oDu0QE9fhOfxPnXFqK15MRQXd0Wrn3xkW8vg+4b2frrTHRSboZOJJwopsGDAXeATzROecQpSOqKSuaBudpwDHAKjO7AOgJNEpoVM65uiPoukazJUs0p95iZjuA7ZKaA6uBOn3D8IxXX6Nr/8Po0ncAt99zb4XjZsZVv/0dXfoOoOego5j/0cLSY3f980H2P2wQBww8gtGXjGXr1q21GXqtyT2uC2d8ejUjP/8lPa+rOFnNgb8eyCnzL+eU+Zdz2sIrubj4Fhrt1hiAIx49mXNWXc9pC6+s7bBr1Yz33qf7KWew34jT+OtjFTs4T06bwUEjz+KgkWcx8IJL+OjzpaXHLv7jn2j3s6H0PGN0hc/VSRmKbktWeFGUmSupJfAw4ZHY+cCcmj4kqb2k1yV9KmmRpKtjCzU+QqEQV1z3W6Y9/SSL3n2Lic+/wOIlS8qUmf7KqyxbvpzP57zPg3+/k59fex0ABUVF3PvwI+S9MpOP33mT0I4QE194MQnfIrGUIQbeN5zpwx7n2R7/oPOoA2jZrXWZMgvvfIfne9/P873vZ84Nsyh680v+t34LAEsmfMi0of+qrOq0EQqFuOr2O5nyj7v4+L9P8fTMWSxevqJMmQ452bz28D/58Okn+N3FFzD2T38pPXbu8OOZeu9dtR12wtT5+ejM7OdmtiG4IDgYOC/owtZkO/ArM+sGDAAul9Q9tnBjN2f+h3Tu2JFOHfamYcOGjDz5JCZNn1mmzKTpMznnjDOQxIA+B7Nh43cUrfoGgO3bQ2zZupXt27ezefMWstu2TcbXSKjW/XLZuGwd369Yz47iEF88/TEdRnSrsnznUQfyxcQfW72r3v6S/327pTZCTZo5ixazT/tcOuXm0LBBA844djCT33irTJlDex7Ibs2bAzDggP0pWL2m9Nig3gexe4vmtRpzwtTlrquk3uU3YHcgK3hdLTMrMrP5wevvCU/tVNnjHLWqoKiI3Ozs0v3c7HYUFBWVKVNYVET7nIplctq141eXX8bevQ4mu8eBtGjenGOPOrKWIq89u+Y0Z1P+xtL9TfnfsWtO5X8oMxs3IHdIF1Y8t6i2wksJhavX0H7PNqX7uXu2oXDNmirLj3/xJYYcOqA2Qqt1YqceAUuK6kZd/1bNMQOOjvYkkjoABwGzKzl2KXApwF65udFW+ZNVdstN+eljqiqzfsMGJk+fwfJ5c2jZogVnXHgJ/3nmv5x9xmkJizcpKvn/sapblfYevh/fvPt1abe1vqj8/5HKy76eN4/HJk3mzUcfSnBUSSKiuwiWRNXdMHxUPE4gqSnwHPALM/uukvM8BDwE0KdXzxpv/ItVbnY2+YU/PlWSX1hUofuZk53NyoKKZV558y067L0XrVuFnzg5+YRhvJeXl3aJblP+d+ya26J0f9fc5mwu/L7SsvuMPJBlEd3W+iJnzzas/GZ16X7+N6tp16p1hXILly5lzK1/Zsq9d7FHyxYVjqeNur44TiwkNSCc5J4ws+cTea5o9T2oF0uXL2fFV1+xbds2nn7hRU4ccmyZMicOOZZ/P/MMZsYHc+fRonkz2rXdk71yc5k9dx6bN2/GzHjtrbfptm+NzxPXOWvyCmjRZQ+addiNjAaZ7DPyAL6a/FmFcg2aN6LdER34alL9m3C6b/duLFu5khUFhWwrLuaZWS8z/IjDy5T5umgVp//6t0y49Wb23XuvJEVaG6IciUjiaERUT0b8FAr3Bx8FPjWzvyfqPDsrKyuLe2//M0NOP5PQjhAXjD6THl27Mu6x8Cjh2AvOY9jgnzHtlVfp0ncATRo3Zvw/7gag/8G9OXX4CRx89LFkZWVy0AEHcOm55yTx2ySGhXbw7pVTGDrjPDIyM1jy2DzWL15NtzF9Afj0wTwAOp7cnYJZy9i+ubjM549+4gyyj+zILq2aMPrra5n3h9dYMn5erX+PRMrKyuKe3/yaYVdcTSi0g/NHnECPfTrx4H/Df5+POe0U/vTwo6zbuJErb78j/JnMTGb/ZwIAZ93we96cO5+1Gzaw99Dh3DzmEi486cRkfZ3Y1IGuq6J4TOynVSwNBN4GPgZ2BG/fEDziUak+vXpa3quzEhJPOni4dcV7/lxZF80dnuwQUlr/s89n7uJP49q0OnjPTvbu6Ojm+Gh899nzqpmmKWGieQRMhKdS72Rmt0jaC2hrZtXeS2dm71D5/FLOuXQiUBo8AvYAcAhwZrD/PeF53Z1zLizFn4yI5hpdfzPrLelDADNbHyx76JxzdeIaXTSJrjhYnccAJLXmx2tuzjmX8itYR5Po/gG8ALSRdBvh2UxuTGhUzrk6JLnd0mhEs67rE5LmEZ6qScBJZlb/bpxyzlWuDnRdo1nXdS9gM/ASMBnYFLznnHNhmYpuq4GkIZKWSFom6fpqyvWVFJIU1WNJ0XRdp/LjIjm7AB2BJUBU6yk659KciEvXNRgLuJ/wLEn5QJ6kyWa2uJJyfyW8tkRUoum6HlDuJL0puyKYc66+i881un7AMjNbDiBpIjACWFyu3JWEHy3tG3V4OxtJMPVS1CdwzqW/nXjUtZWkuRHbpRHV5AArI/bzKTe1m6Qc4GRgHDshmicjronYzQB6A1VPvOWcq192ruu6tppHwCqrpPwzqncD15lZqPz0atWJ5hpds4jX2wlfs3su6jM459KcICsuXdd8oH3Efi5QWK5MH2BikORaAcMkbTezF6uruNpEF1z0a2pm1+5sxM65ekLE64bhPKCLpI5AATAKKLN6kJl1LD2tNAGYUlOSg2oSnaSsYOXsGqdNd87Vc3G4jy7IN1cQHk3NBMab2SJJY4PjO3VdLlJ1Lbo5hK/HLZA0GXgW2BQRVEpMpOmcSwFxejIimMZtWrn3Kk1wZnZ+tPVGc41ud2Ad4TUiSu6nM8ATnXMubvfRJVJ1ia5NMOL6CT8muBIJX9vBOVeHpHaeqzbRZQJNiW7I1zlXX0mQldoPu1aX6IrM7JZai8Q5V2el+CxN1Sa6FA/dOZcy6vA1umNqLQrnXN1VlwcjzOzb2gzEOVeHpfYlusSt6+qcq0dS/CKdJzrnXGxEVJNqJpMnOudc7FI7z3mic87FSHiic86luzRYBcw552qU2nnOE51zLkZ1+T4655yLVorfXeKJzjkXB57odkJmJtp112RHkbIuemdwskNIeff0mZTsEFLaN7YhMRWneNc1xR/ccM6lPO3EVlNV0hBJSyQtk3R9JcdHSFooaUGwXOLAaEJMrRadc65uikODLliM635gMOEVwfIkTTazyAWsXwUmm5lJOhB4BuhaU93eonPOxS5D0W3V6wcsM7PlZrYNmAiMiCxgZj+YWcnEv7sS5STAnuicc7ER4UwSzQatgi5nyXZpRE05wMqI/fzgvbKnk06W9BnhNaYvjCZE77o652IX/f0la82sT1W1VPJehRabmb0AvCBpEHAr8LOaTuotOudc7OIzGJEPtI/YzwUKqypsZm8B+0hqVVPFnuicc7GLT6LLA7pI6iipITAKmFzmNFJnKdx8lNQbaEh4OdZqedfVORcjxeXRCDPbLukKYCbhVQjHm9kiSWOD4+OAU4FzJRUDW4CREYMTVfJE55yLjQinpTgws2nAtHLvjYt4/Vfgrztbryc651zsUvxhV090zrnYpXae80TnnIuN4nOJLqE80TnnYpfiD/V7onPOxS6185wnOudcjHyGYedcvZDaec4TnXMuDrxF55xLe6md5zzROediVAfuL/FE55yLnVJ7fhBPdM652KV2g84TnXMuDrzr6pxLe57onHNpzQcjnHP1gic651zaS/FR19SOzjlXN5R0X2vaaqxGQyQtkbRM0vWVHD9L0sJge09Sz2jC8xadcy5G8blGJykTuB8YTHhFsDxJk81scUSxFcARZrZe0lDgIaB/TXV7onPOxUbE6xpdP2CZmS0HkDQRGAGUJjozey+i/AeEl0SskXddnXOxi77r2krS3Ijt0ohacoCVEfv5wXtVuQiYHk143qJzztWmtWbWp4pjlTULK13KUNJRhBPdwGhO6onOORczZcSlc5gPtI/YzwUKK5xLOhB4BBhqZjUuXg31NNHNmPUyV//6N4RCO7j4/HO5/tpflTluZlz9q98wbeYsmjRpzISHxtH7oF5s3bqVQT8bwv+2/Y/t27dz2skn8cff/y5J3yKxZnwwm2vuvo/QjhAXDj+e6845q8zxJ2e+zB1PPAXAro0bc/+vf0nPLp1Lj4dCIfpfNIbs1q2YfMfttRp7belwXBeOvPsEMjIz+PjRPPL++laZ431+fThdR4cHBTOyMtm9W2vGtbmN4s3FjHzzEjIbZaGsDJY+9wnv/+HVZHyFOInbDcN5QBdJHYECYBQwusyZpL2A54FzzOzzaCtOWKKTtAvwFtAoOM9/zezmRJ0vWqFQiMt/8StenjqJ3Jwc+g48ghNPOJ7u3bqWlpk+cxZLv/iCpZ8sYPacPC676pfMfvt1GjVqxGszptC0aVOKi4sZePSxDD12MAP690viN4q/UCjEVX+7hxl330lum9YMuHgswwceRveOHUrLdMhux2v33cNuzZsx/f3ZjP2/v/H+w/8sPf6PZ5+ja4e9+W7TpiR8g8RThjj6vhN57tjxfJ//HWfN+TlfTP6Mbz9dXVpm7p1vM/fOtwHodEJXev/iMLau3wLAs8c8SvGmbWRkZTDy7TF8Of1zimavrPRcKS9OgxFmtl3SFcBMwktijzezRZLGBsfHATcBewAPKHzO7dV0hUslcjDif8DRZtYT6AUMkTQggeeLypy8uXTepxOdOnakYcOGjDr9VCZNmVKmzKQpUzl39JlIYkD/fmzYuIGiolVIomnTpgAUFxdTvL0Ypfgd4T/FnE8/Y5/cHDrlZNOwQQPOOOZoJr/9bpkyhx6wP7s1bwbAgB7dKVi9pvRY/urVTHvvAy4cfnytxl2b2vbLZcOydWxcsZ4dxSE+e3oh+4zoVmX5rqN6smTiR6X7xZu2AZDRIJOMBhmYVXopqu6I0310ZjbNzPY1s33M7LbgvXFBksPMLjaz3cysV7DVmOQggYnOwn4IdhsEW9L/axYUFtE+98eBnNycHAoKisqVKaxYpjB8qSAUCtGr/6G02asTg48+iv79+tZO4LWocM0a2rdpXbqf26Y1hWvWVFl+/JSpDBnwY6v2mnvu4/afjyEjDf8SKNE0pwXf528s3f8hfyPNcppXWjarcQM6DOnC0ucWlb6nDHH2/CsY+80NfP3KMlbNyU94zAkVp0SXKAm9vURSpqQFwGrgZTObXUmZS0uGmtesWZvIcAAq/ZuzfKusujKZmZksmP0e+cs+Y87ceXyyaHGFsnVdZY2Lqlqur8/7kMemTOMvPx8DwJR336PNbrtxcNf9Ehli8lXyc1TVKOs0vCsF735V2m0FsB3Gf3rfx8Pt/0rbvu3Zo8eeCQq0Nij8CFg0W5Ik9MxmFjKzXoRHT/pJ2r+SMg+ZWR8z69O6datEhgNAbk42K/MLSvfzCwrIzm5brkxOxTLt2pUp07JlS44cdDgzZr2c2ICTIKdNa1aW6YquoV2riv9tFi77gjG338Hzt9/GHi1aAPDewk946Z132efUkZx18y28Pu9Dzv3jn2ot9tryQ/5GmuW2KN1vmtuCHwq/q7Rs15EHsmTiwkqP/W/jVla+uZwOQ7okJM5aU59bdCXMbAPwBjCkNs5Xnb59Dmbpsi9Y8eWXbNu2jYnPPseJx5e9lnTi8cN4/MmnMDM+mD2HFs1b0K5dW9asWcOGDRsA2LJlC6+89jpd99s3Cd8isfp23Y9l+fmsKCxiW3Exz7z6GsMHHlqmzNervuH0G37PhJtuYN+9frwj4M+XXcpXL/6XL557mif+eBNHHXwQj998Y21/hYRblVdAyy6taN5hNzIaZNJ15IEsn/xphXINmzci94iOLJv0Y8u/catdadRiFwCydslir2M68+1nVV8aSHklgxEpnOgSOeraGig2sw2SGgM/A/6aqPNFKysri/vuupPjhp9EKLSDC887hx7duzHu4UcBGHvJRQwbchzTZs6ic4+eNGnSmMceDI8mFq36hvMuGUMoFGLHjh2cceopnDBsaDK/TkJkZWVxzy+vZtg11xIK7eD8E4bSo1NHHnxhEgBjTh7Bnx77F+u++44r77wr/JnMTGaPfyiZYdcqC+3g9Ssnc+qMC1Cm+OSxeaxbvJoDx4SvVS58cA4AnU/uwZezlrF9c3HpZ3dt14whE05DmUIZGXz+7MesmLokKd8jblL8cqwSNdoT3NT3L8LDxBnAM2Z2S3Wf6XNwb5v77lvVFanXQvPnJTuElHfPwJnJDiGl3WWPsdKK4pqW+hywv8158bmoymZ27jov2pHSeEpYi87MFgIHJap+51wKSfER9nr5ZIRzLp6U8hNveqJzzsUmftM0JYwnOudc7DzROefSXoonutTuWDvnXBx4i845FyNf19U5l+4EZHiic86lPU90zrl0l+JdVx+McM7FTlFuNVVT8wLWXSW9L+l/kn4dbXjeonPOxUxx6LpGuYD1t8BVwEk7U7e36JxzMYpyiqaau7elC1ib2TagZAHrUma22szygOLKKqiKJzrnXGxKRl2j2aq3swtYR827rs65OIi669pK0tyI/YfMrGQiw6gXsN5Znuicc7GLftR1bTXz0UW1gPVP4V1X51zs4jPqWrqAtaSGhBewnhyP8LxF55yLg9pZwFpSW2Au0BzYIekXQHczq3xlooAnOudcjKIaaIiKmU0DppV7b1zE61WEu7Q7xROdcy42PvGmc65+8ETnnEt3qZ3nPNE55+LAu67OubTnic45l9Yk5InOOZf2PNE559KfJzrnXLpL7Tznic45FwfedXXOpTdf7tA5l+7qwCNgMovLvHZxIWkN8FWy44jQClib7CBSmP8+NUu132hvM2sdzwolzSD8PaOx1syGxPP80UipRJdqJM2tZpLAes9/n5r5b5QafOJN51za80TnnEt7nuiq91DNReo1/31q5r9RCvBrdM65tOctOudc2vNE55xLe57onHNpzxNdFSRlJjuGVCWps6Q+kholO5ZUJamHpCMk7ZHsWJwnugok7QtgZiFPdhVJOgF4HrgDmFDye7kfSRoKPAX8Eng8WIvUJZEnugjBH+IFkp4ET3blSToUuBM4z8yOAtYD1yc3qtQi6UjgHuBiMzsJ2Absn8SQHH57SSlJuwLPEW6tHApkmdnZwbFMMwslM75UECS6fc1sQrDfGngYGGlm/0tmbKlCUjegrZm9HrTk5gNzgG+Al4HnzP/Q1TpPdBEkZQPfAbsA44CtJcnOlV633NXMvgtetwNeAo41szWS9jCzdcmNMnVI+h3hP2N/knQBMAS4wszWJDm0escTXRWCi8gPAVvM7GxJvYHNZvZZkkNLCZKyCP+FMMnMjpF0FjAQuMbMtiQ3utQkaRpwo5nNT3Ys9Y1fo6tC0DIZAxRL+gx4GvghuVGlDjPbbmY/ACsl/QW4BnjAk1yYyi2LJelUYE+gMDkR1W8+8WY1zGytpIXAUGCwmeUnO6ZUEfxBbgAcHvz7GDNbmtyoUkfJdbjgFpyzCf9FMNLMViU1sHrKE101JO0GDCN8DerjZMeTSoI/yNsk3QrkeZKr0g6gCDjFzJYkO5j6yq/R1UDSLma2NdlxpCpJ8lFEl+o80Tnn0p4PRjjn0p4nOudc2vNE55xLe57onHNpzxNdHSIpJGmBpE8kPSupSQx1TZB0WvD6EUndqyl7ZPCc686e40tJFdb7rOr9cmV26uZsSX+Q9OudjdHVD57o6pYtZtbLzPYnPCvG2MiDP3WmFTO72MwWV1PkSMITHThXJ3miq7veBjoHra3Xg6mlPpaUKekOSXmSFkoaA+H73STdJ2mxpKlAm5KKJL0hqU/weoik+ZI+kvSqpA6EE+ovg9bk4ZJaS3ouOEeepMOCz+4haZakDyU9CIgaSHpR0jxJiyRdWu7Y34JYXg1mSkHSPpJmBJ95W1LXuPyaLq35kxF1UPBA/VBgRvBWP2B/M1sRJIuNZtY3ePzoXUmzgIOA/YADCD9zuRgYX67ekmmXBgV17W5m30oaB/xgZncG5Z4E7jKzdyTtBcwEugE3A++Y2S2SjgfKJK4qXBicozGQJ+m54DnjXYH5ZvYrSTcFdV9BeKKFsWa2VFJ/4AHg6J/wM7p6xBNd3dJY0oLg9dvAo4S7lHPMbEXw/rHAgSXX34AWQBdgEPBUMK9eoaTXKql/APBWSV1m9m0VcfwM6B7x3HpzSc2Cc5wSfHaqpPVRfKerJJ0cvG4fxLqO8KNTTwfv/wd4XlLT4Ps+G3Fun87d1cgTXd2yxcx6Rb4R/IHfFPkWcKWZzSxXbhhQ02MwiqIMhC95HFJ+ppIglqgftQlm4/1ZUNdmSW8QnvqpMhacd0P538C5mvg1uvQzE7hMUgMIr4ERzJ78FjAquIbXDjiqks++DxwhqWPw2d2D978HmkWUm0W4G0lQrlfw8i3grOC9ocBuNcTaAlgfJLmuhFuUJTKAklbpaMJd4u+AFZJOD84hST1rOIdznujS0COEr7/Nl/QJ8CDhlvsLwFLgY+CfwJvlPxjMfHsp4W7iR/zYdXwJOLlkMAK4CugTDHYs5sfR3z8CgyTNJ9yF/rqGWGcAWcFUWLcCH0Qc2wT0kDSP8DW4W4L3zwIuCuJbBIyI4jdx9Zw/1O+cS3veonPOpT1PdM65tOeJzjmX9jzROefSnic651za80TnnEt7nuicc2nv/wHnBrQUeu7GdAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "normalize = True\n",
    "cmap = 'RdPu'\n",
    "classes = [1,2,3]\n",
    "title = 'Cofusion Matrix'\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "ax.figure.colorbar(im, ax = ax)\n",
    "ax.set(xticks = np.arange(cm.shape[1]), yticks = np.arange(cm.shape[0]), xticklabels = classes, yticklabels = classes, ylabel = 'True label', xlabel = 'Predicted label', title = title)\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha = 'right', rotation_mode = 'anchor')\n",
    "fmt = '.2f' if normalize else 'd'\n",
    "thresh = cm.max() / 2\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(j, i, format(cm[i, j], fmt), ha = 'center', va = 'center', color = 'white' if cm[i,j] > thresh else 'black')\n",
    "        fig.tight_layout()\n",
    "# plt.savefig('figures/VAE_CM.jpg')"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-9.m82",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m82"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
